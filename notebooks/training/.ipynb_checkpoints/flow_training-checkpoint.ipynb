{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a076340e-f762-43f8-83b3-d57bf4f43af2",
   "metadata": {},
   "source": [
    "# Training of the flow matching model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b027b-4538-46d1-8106-512436554998",
   "metadata": {},
   "source": [
    "In this script we will train the flow matching model.\n",
    "For this purpose we will use the training data already encoded in a 50 dimension space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676047f1-cc46-491a-a20d-b0d1c6362468",
   "metadata": {},
   "source": [
    "- Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1207d54f-991f-42c1-8874-175aafd89f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from abc import ABC, abstractmethod\n",
    "from typing import Optional #, List, Type, Tuple, Dict\n",
    "import math\n",
    "import anndata as ad\n",
    "#import numpy as np\n",
    "#from matplotlib import pyplot as plt\n",
    "#import matplotlib.cm as cm\n",
    "#from matplotlib.axes._axes import Axes\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "#from torch.func import vmap, jacrev\n",
    "#from tqdm import tqdm\n",
    "#import seaborn as sns\n",
    "#from sklearn.datasets import make_moons, make_circles\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c52a8-12c0-4707-85c0-b2957b475d9b",
   "metadata": {},
   "source": [
    "- Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98a63f9-469b-4f22-9da0-a88d91f27b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of latent space: (2110, 50)\n",
      "[[-1.4144031   1.3279299   0.757624   ...  1.9632016   3.2006185\n",
      "   1.0286264 ]\n",
      " [-3.0197775  -1.5338613   1.2328798  ... -1.762154    3.0394304\n",
      "  -0.13480194]\n",
      " [-2.9895246  -0.5492041   0.01084488 ...  1.2618108  -1.4493724\n",
      "   0.35208455]\n",
      " ...\n",
      " [ 1.2099221   0.04875063 -0.70176685 ...  1.5337447   1.6093717\n",
      "  -0.77522033]\n",
      " [ 0.16601333  0.13243417  1.3788003  ...  0.4668453  -1.8225284\n",
      "   0.20945628]\n",
      " [-2.644291    0.13521816  0.795208   ... -0.2699267   1.9044281\n",
      "  -2.3405967 ]]\n"
     ]
    }
   ],
   "source": [
    "# Load the encoded data from the autoencoder\n",
    "data_dir = \"/dtu/blackhole/1e/213566/data/datasets/pbmc3k/\"\n",
    "input_file_path = data_dir + \"pbmc3k_train_with_latent.h5ad\"\n",
    "adata = ad.read_h5ad(input_file_path)\n",
    "\n",
    "# Access latent representation\n",
    "latent = adata.obsm[\"X_latent\"]\n",
    "# make it to a tensor and save in GPU\n",
    "latent_tensor = torch.tensor(latent, dtype=torch.float32, device = device)\n",
    "print(\"Shape of latent space:\", latent.shape)\n",
    "print(latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b15f8bc-9816-4525-8bee-a6df32baf3fd",
   "metadata": {},
   "source": [
    "### Defining the flow model classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2569d01-79f5-44c9-a09f-e2d82049488e",
   "metadata": {},
   "source": [
    "- Training data **empirical distribution** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d9554cd-e92b-4ffd-a307-89cfcadca6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmpiricalDistribution(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: torch.Tensor, #data inputted must be a torch tensor\n",
    "        bandwidth: Optional[float] = None,\n",
    "        compute_log_density: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert data.dim() == 2, \"data must be shape (N, D)\"\n",
    "        data = data.contiguous()\n",
    "        \n",
    "        self.register_buffer(\"data\", data)   # (N, D)\n",
    "        self.n = data.shape[0]\n",
    "        self.data_dim = data.shape[1]        # <-- renamed attribute\n",
    "        self.compute_log_density_flag = compute_log_density\n",
    "\n",
    "        # Bandwidth estimation\n",
    "        if bandwidth is None:\n",
    "            std = torch.std(data, dim=0).mean().item()\n",
    "            factor = (4.0 / (self.data_dim + 2.0)) ** (1.0 / (self.data_dim + 4.0))\n",
    "            bw = factor * (self.n ** (-1.0 / (self.data_dim + 4.0))) * (std + 1e-6)\n",
    "            self.bandwidth = torch.tensor(float(bw), device=self.data.device)\n",
    "        else:\n",
    "            self.bandwidth = torch.tensor(float(bandwidth), device=self.data.device)\n",
    "\n",
    "        self._log_const = -0.5 * self.data_dim * math.log(2.0 * math.pi) - self.data_dim * torch.log(self.bandwidth).item()\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.data_dim\n",
    "    def sample(self, num_samples: int) -> torch.Tensor:\n",
    "        idx = torch.randint(0, self.n, (num_samples,), device=self.data.device)\n",
    "        return self.data[idx]\n",
    "\n",
    "    def log_density(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.compute_log_density_flag:\n",
    "            raise RuntimeError(\"log_density disabled (compute_log_density=False).\")\n",
    "\n",
    "        assert x.dim() == 2 and x.shape[1] == self.data_dim\n",
    "\n",
    "        x = x.to(self.data.device)\n",
    "        x_norm2 = (x ** 2).sum(dim=1, keepdim=True)\n",
    "        data_norm2 = (self.data ** 2).sum(dim=1).unsqueeze(0)\n",
    "        cross = x @ self.data.t()\n",
    "        d2 = x_norm2 + data_norm2 - 2.0 * cross\n",
    "\n",
    "        sigma2 = (self.bandwidth ** 2).item()\n",
    "        exponents = -0.5 * d2 / (sigma2 + 1e-12)\n",
    "        lse = torch.logsumexp(exponents, dim=1, keepdim=True)\n",
    "\n",
    "        log_prob = math.log(1.0 / self.n) + lse + self._log_const\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f72b90-9672-44bf-9358-24f026c2ea3a",
   "metadata": {},
   "source": [
    "Testing of empirical distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd6c723-2603-4ee4-a2e4-4444bf07beb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4393, -2.3863, -0.7780,  0.1227,  1.2990, -1.1641,  2.1897,  3.7628,\n",
      "         -2.1646,  1.0393, -0.6197, -1.8223, -0.9345,  1.3294, -0.6704, -2.1641,\n",
      "          0.8100, -1.4776, -1.0023,  0.2418, -1.0109, -1.6613, -0.3802, -1.7112,\n",
      "          0.5353,  1.2151,  3.5505,  1.8725, -0.9967, -1.2711, -1.0995, -3.1594,\n",
      "         -0.7899,  2.2381, -1.1912,  1.7589,  0.4701,  1.8941,  0.5098, -0.5628,\n",
      "          0.7041,  1.6715,  1.7624, -4.8589, -1.3353,  0.3684,  0.0531, -3.1328,\n",
      "         -0.0286, -0.0980],\n",
      "        [-1.1384, -0.5498,  3.7593, -1.6126,  1.9368, -2.4993,  1.2149,  0.5458,\n",
      "          1.2813,  1.0450, -0.0913,  0.8888, -0.5665,  1.8759, -0.3457, -1.4163,\n",
      "         -3.3777,  3.3526,  2.4735, -0.5274,  0.9720,  1.0457,  1.4560,  0.2047,\n",
      "          0.6509,  2.4678,  0.5657,  1.0632,  0.5887,  1.0976, -1.2962,  0.8395,\n",
      "         -1.6024, -0.9698, -1.9928,  0.7742,  1.2832,  1.3159, -2.5379,  1.7630,\n",
      "          1.5861,  0.4522, -2.0793,  1.6866, -0.2321, -0.8127,  2.1127,  0.3067,\n",
      "          1.6832, -0.4299],\n",
      "        [-0.2010, -2.4080, -0.3458,  3.3831, -0.1584, -1.8141,  0.2586,  0.6598,\n",
      "          0.6524, -1.5951,  2.5289, -0.5495,  3.5249,  1.6956,  1.0989, -0.2569,\n",
      "          1.9938,  1.9791,  0.2529,  4.1238,  0.3293,  0.8208,  1.4745, -0.5884,\n",
      "         -0.4572, -1.4678,  2.0506, -1.3677, -0.4806, -1.3806, -1.9246,  1.6166,\n",
      "          0.3179,  2.4813, -1.3067,  1.2804,  1.2695,  1.5608,  2.9185,  1.9926,\n",
      "         -0.6051,  2.0221, -0.5717, -0.7481,  2.2398,  3.4186,  0.3946, -1.6153,\n",
      "          0.4505,  1.0004]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# lets test if the empirical distribution class actually works by sampling from it\n",
    "\n",
    "dist = EmpiricalDistribution(latent_tensor)\n",
    "samples = dist.sample(3)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf21e2b-de0f-424d-95a6-e50ed4c990ee",
   "metadata": {},
   "source": [
    "- **Gaussian distribution** class to draw from the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cbc68f2-372e-428d-b98e-4d1f3a4d9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Multivariate Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, mean: torch.Tensor, cov: torch.Tensor):\n",
    "        \"\"\"\n",
    "        mean: shape (dim,)\n",
    "        cov: shape (dim,dim)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", mean)\n",
    "        self.register_buffer(\"cov\", cov)\n",
    "\n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self.mean.shape[0]\n",
    "\n",
    "    @property\n",
    "    def distribution(self):\n",
    "        return D.MultivariateNormal(self.mean, self.cov, validate_args=False)\n",
    "\n",
    "    def sample(self, num_samples) -> torch.Tensor:\n",
    "        return self.distribution.sample((num_samples,))\n",
    "        \n",
    "    def log_density(self, x: torch.Tensor):\n",
    "        return self.distribution.log_prob(x).view(-1, 1)\n",
    "\n",
    "    @classmethod\n",
    "    def isotropic(cls, dim: int, std: float) -> \"Gaussian\":\n",
    "        mean = torch.zeros(dim)\n",
    "        cov = torch.eye(dim) * std ** 2\n",
    "        return cls(mean, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb31e9-4b1e-4769-b8d6-a4a0590edc17",
   "metadata": {},
   "source": [
    "- **Alpha** and **beta** classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f10e95-090c-43e7-89be-1b8563cdfc1e",
   "metadata": {},
   "source": [
    "We use $\\alpha_t$ and $\\beta_t$ to schedule the noise in the Gaussian probability paths.\n",
    "\n",
    "These are two continuously differentiable, monotonic functions that follow:\n",
    "\n",
    "$\\alpha_0$ = $\\beta_1$ = 0     and     $\\alpha_1$ = $\\beta_0$ = 1.\n",
    "\n",
    "We have chosen a linear form with a simple derivative:\n",
    "\n",
    "$\\alpha_t$ = t ; $\\alpha_t$' = 1\n",
    "\n",
    "$\\beta_t$ = 1 - t ; $\\beta_t$' = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3433c200-5e40-4dc6-8fca-201b13c7b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to go with Gaussian probability path, therefore we need to load functions for alpha and beta\n",
    "class LinearAlpha():\n",
    "    \"\"\"Implements alpha_t = t\"\"\"\n",
    "    \n",
    "    def __call__(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        return t  # linear in time\n",
    "\n",
    "    def dt(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.ones_like(t)  # derivative of t is 1\n",
    "\n",
    "\n",
    "class LinearBeta():\n",
    "    \"\"\"Implements beta_t = 1 - t\"\"\"\n",
    "    \n",
    "    def __call__(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        return 1 - t\n",
    "\n",
    "    def dt(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        return -torch.ones_like(t)  # derivative of 1 - t is -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050fb6b-4c77-4587-9f4a-c993f6e7431d",
   "metadata": {},
   "source": [
    "- **Gaussian probability path** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ba9fdd-c53b-4e8e-b80d-a142773b04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianConditionalProbabilityPath():\n",
    "    def __init__(self, p_data, alpha, beta):\n",
    "        self.p_data = p_data \n",
    "        p_simple = Gaussian.isotropic(p_data.dim, 1.0)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def sample_conditioning_variable(self, num_samples: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples the conditioning variable z ~ p_data(x)\n",
    "        Args:\n",
    "            - num_samples: the number of samples\n",
    "        Returns:\n",
    "            - z: samples from p(z), (num_samples, dim)\n",
    "        \"\"\"\n",
    "        return self.p_data.sample(num_samples)\n",
    "    \n",
    "    def sample_conditional_path(self, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the conditional distribution p_t(x|z) = N(alpha_t * z, beta_t**2 * I_d)\n",
    "        Args:\n",
    "            - z: conditioning variable (num_samples, dim)\n",
    "            - t: time (num_samples, 1)\n",
    "        Returns:\n",
    "            - x: samples from p_t(x|z), (num_samples, dim)\n",
    "        \"\"\"\n",
    "        return self.alpha(t) * z + self.beta(t) * torch.randn_like(z)\n",
    "        \n",
    "    def conditional_vector_field(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluates the conditional vector field u_t(x|z)\n",
    "        Note: Only defined on t in [0,1)\n",
    "        Args:\n",
    "            - x: position variable (num_samples, dim)\n",
    "            - z: conditioning variable (num_samples, dim)\n",
    "            - t: time (num_samples, 1)\n",
    "        Returns:\n",
    "            - conditional_vector_field: conditional vector field (num_samples, dim)\n",
    "        \"\"\" \n",
    "        alpha_t = self.alpha(t) # (num_samples, 1)\n",
    "        beta_t = self.beta(t) # (num_samples, 1)\n",
    "        dt_alpha_t = self.alpha.dt(t) # (num_samples, 1)\n",
    "        dt_beta_t = self.beta.dt(t) # (num_samples, 1)\n",
    "\n",
    "        return (dt_alpha_t - dt_beta_t / beta_t * alpha_t) * z + dt_beta_t / beta_t * x\n",
    "\n",
    "    def conditional_score(self, x: torch.Tensor, z: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluates the conditional score of p_t(x|z) = N(alpha_t * z, beta_t**2 * I_d)\n",
    "        Note: Only defined on t in [0,1)\n",
    "        Args:\n",
    "            - x: position variable (num_samples, dim)\n",
    "            - z: conditioning variable (num_samples, dim)\n",
    "            - t: time (num_samples, 1)\n",
    "        Returns:\n",
    "        - conditional_score: conditional score (num_samples, dim)\n",
    "        \"\"\" \n",
    "        alpha_t = self.alpha(t)\n",
    "        beta_t = self.beta(t)\n",
    "        return (z * alpha_t - x) / beta_t ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9d082-03e5-4938-bdda-e4648270bddc",
   "metadata": {},
   "source": [
    "- **Marginal vector field** class using the Euler method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eeef4d-a9d8-4815-af8c-fec7c41e3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we somehow want to model the marginal vector field from the conditonal vector field\n",
    "# for that we will use eulers:\n",
    "class EulerSimulator():\n",
    "    def __init__(self, ode, z: torch.Tensor, u_mean, u_std):\n",
    "        self.ode = ode\n",
    "        self.z = z\n",
    "        self.u_mean = u_mean\n",
    "        self.u_std = u_std\n",
    "\n",
    "    def step(self, xt: torch.Tensor, t: torch.Tensor, h: float):\n",
    "        # Expand z to match batch size\n",
    "        if self.z.shape[0] == 1:\n",
    "            z_exp = self.z.expand(xt.shape[0], -1)\n",
    "        else:\n",
    "            z_exp = self.z\n",
    "\n",
    "        # Get normalized drift from model\n",
    "        dx_norm = self.ode.drift_coefficient(xt, t, z_exp)\n",
    "\n",
    "        # Un-normalize to match the real vector field scale\n",
    "        dx = dx_norm * self.u_std + self.u_mean\n",
    "\n",
    "        # Euler update\n",
    "        return xt + dx * h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48117493-9b52-4e5d-8a72-e02524a9863f",
   "metadata": {},
   "source": [
    "- **Time** embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9800bb9-9fb3-495e-8bf0-cd054df473d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedder(nn.Module):\n",
    "    def __init__(self, embed_dim=64, max_freq=1e4):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_freq = max_freq\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embed_dim*2, embed_dim*2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embed_dim*2, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        freqs = torch.exp(torch.linspace(0, math.log(self.max_freq), self.embed_dim // 2, device=t.device))\n",
    "        args = t * freqs\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "        return self.mlp(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad5cbfd-4285-4c37-8794-e7388673bbf7",
   "metadata": {},
   "source": [
    "- **ResNetBlock** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a9677-716a-4fea-b001-cd68903d73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim=None):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or dim*2\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192768de-0061-4c9e-a836-6fee09b422b6",
   "metadata": {},
   "source": [
    "- **Neural vector field** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "950fe7c8-431a-491c-8f30-f127d786ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralVectorField(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim=256, n_resblocks=5, time_embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.x_proj = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.z_proj = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.time_embedder = TimeEmbedder(time_embed_dim)\n",
    "\n",
    "        self.resblocks = nn.ModuleList([\n",
    "            ResNetBlock(hidden_dim*2 + time_embed_dim, hidden_dim*2) for _ in range(n_resblocks)\n",
    "        ])\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2 + time_embed_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z, t):\n",
    "        xh = self.x_proj(x)\n",
    "        zh = self.z_proj(z)\n",
    "        th = self.time_embedder(t)\n",
    "        h = torch.cat([xh, zh, th], dim=-1)\n",
    "        for block in self.resblocks:\n",
    "            h = block(h)\n",
    "        return self.output_layer(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d5d2d-a2f8-4f79-b703-bac71b375480",
   "metadata": {},
   "source": [
    "## Hyperparameters and initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd85d3-d1e3-48ae-9775-5b49006d297e",
   "metadata": {},
   "source": [
    "- Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5c11f-6ca6-4f7f-8e9d-8085c690337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "num_epochs = 10000\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79aefe2-7e3e-41cd-a8fa-569216dadcb0",
   "metadata": {},
   "source": [
    "- Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b44fee9-b80d-4755-ba62-3f83f82a5d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.GaussianConditionalProbabilityPath object at 0x7f59a9924320>\n"
     ]
    }
   ],
   "source": [
    "emp_dist = EmpiricalDistribution(latent_tensor)\n",
    "alpha = LinearAlpha()\n",
    "beta = LinearBeta()\n",
    "latent_dim = latent_tensor.shape[1] # 50 in our case\n",
    "vf_model = NeuralVectorField(latent_dim=latent_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(vf_model.parameters(), lr=learning_rate)\n",
    "# Initialize GaussianConditionalProbabilityPath\n",
    "path = GaussianConditionalProbabilityPath(emp_dist, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba24da0-11a4-4d9b-90bf-6c38cca18444",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6127f93f-e40f-494e-adba-49807386c7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Loss: 1.221615\n",
      "[50] Loss: 0.052373\n",
      "[100] Loss: 0.024603\n",
      "[150] Loss: 0.019758\n",
      "[200] Loss: 0.015473\n",
      "[250] Loss: 0.013621\n",
      "[300] Loss: 0.012283\n",
      "[350] Loss: 0.010856\n",
      "[400] Loss: 0.010001\n",
      "[450] Loss: 0.010703\n",
      "[500] Loss: 0.009139\n",
      "[550] Loss: 0.007841\n",
      "[600] Loss: 0.007962\n",
      "[650] Loss: 0.006073\n",
      "[700] Loss: 0.006870\n",
      "[750] Loss: 0.006649\n",
      "[800] Loss: 0.007105\n",
      "[850] Loss: 0.006489\n",
      "[900] Loss: 0.006736\n",
      "[950] Loss: 0.007168\n",
      "[1000] Loss: 0.005532\n",
      "[1050] Loss: 0.005542\n",
      "[1100] Loss: 0.004399\n",
      "[1150] Loss: 0.005089\n",
      "[1200] Loss: 0.006603\n",
      "[1250] Loss: 0.005839\n",
      "[1300] Loss: 0.005113\n",
      "[1350] Loss: 0.005315\n",
      "[1400] Loss: 0.005272\n",
      "[1450] Loss: 0.005721\n",
      "[1500] Loss: 0.004632\n",
      "[1550] Loss: 0.005760\n",
      "[1600] Loss: 0.004618\n",
      "[1650] Loss: 0.004929\n",
      "[1700] Loss: 0.004659\n",
      "[1750] Loss: 0.004519\n",
      "[1800] Loss: 0.004529\n",
      "[1850] Loss: 0.003832\n",
      "[1900] Loss: 0.004022\n",
      "[1950] Loss: 0.003999\n",
      "[2000] Loss: 0.005106\n",
      "[2050] Loss: 0.003801\n",
      "[2100] Loss: 0.003249\n",
      "[2150] Loss: 0.004419\n",
      "[2200] Loss: 0.004128\n",
      "[2250] Loss: 0.004376\n",
      "[2300] Loss: 0.004379\n",
      "[2350] Loss: 0.003735\n",
      "[2400] Loss: 0.004324\n",
      "[2450] Loss: 0.003437\n",
      "[2500] Loss: 0.003453\n",
      "[2550] Loss: 0.003181\n",
      "[2600] Loss: 0.004775\n",
      "[2650] Loss: 0.003460\n",
      "[2700] Loss: 0.003807\n",
      "[2750] Loss: 0.004117\n",
      "[2800] Loss: 0.004347\n",
      "[2850] Loss: 0.003676\n",
      "[2900] Loss: 0.005438\n",
      "[2950] Loss: 0.003974\n",
      "[3000] Loss: 0.003855\n",
      "[3050] Loss: 0.003406\n",
      "[3100] Loss: 0.002946\n",
      "[3150] Loss: 0.004333\n",
      "[3200] Loss: 0.003425\n",
      "[3250] Loss: 0.003825\n",
      "[3300] Loss: 0.004079\n",
      "[3350] Loss: 0.003238\n",
      "[3400] Loss: 0.003627\n",
      "[3450] Loss: 0.003420\n",
      "[3500] Loss: 0.003335\n",
      "[3550] Loss: 0.003714\n",
      "[3600] Loss: 0.002717\n",
      "[3650] Loss: 0.003696\n",
      "[3700] Loss: 0.003226\n",
      "[3750] Loss: 0.003281\n",
      "[3800] Loss: 0.003454\n",
      "[3850] Loss: 0.003386\n",
      "[3900] Loss: 0.003430\n",
      "[3950] Loss: 0.002230\n",
      "[4000] Loss: 0.002601\n",
      "[4050] Loss: 0.003830\n",
      "[4100] Loss: 0.003950\n",
      "[4150] Loss: 0.003522\n",
      "[4200] Loss: 0.003715\n",
      "[4250] Loss: 0.003462\n",
      "[4300] Loss: 0.003145\n",
      "[4350] Loss: 0.003064\n",
      "[4400] Loss: 0.004064\n",
      "[4450] Loss: 0.003616\n",
      "[4500] Loss: 0.002446\n",
      "[4550] Loss: 0.003106\n",
      "[4600] Loss: 0.003476\n",
      "[4650] Loss: 0.003316\n",
      "[4700] Loss: 0.002733\n",
      "[4750] Loss: 0.003697\n",
      "[4800] Loss: 0.002744\n",
      "[4850] Loss: 0.003623\n",
      "[4900] Loss: 0.002398\n",
      "[4950] Loss: 0.003107\n",
      "[5000] Loss: 0.003592\n",
      "[5050] Loss: 0.003150\n",
      "[5100] Loss: 0.004027\n",
      "[5150] Loss: 0.003393\n",
      "[5200] Loss: 0.002648\n",
      "[5250] Loss: 0.002905\n",
      "[5300] Loss: 0.003076\n",
      "[5350] Loss: 0.002995\n",
      "[5400] Loss: 0.002735\n",
      "[5450] Loss: 0.002744\n",
      "[5500] Loss: 0.002874\n",
      "[5550] Loss: 0.002434\n",
      "[5600] Loss: 0.002842\n",
      "[5650] Loss: 0.002516\n",
      "[5700] Loss: 0.002838\n",
      "[5750] Loss: 0.002592\n",
      "[5800] Loss: 0.002985\n",
      "[5850] Loss: 0.002640\n",
      "[5900] Loss: 0.003052\n",
      "[5950] Loss: 0.002730\n",
      "[6000] Loss: 0.003831\n",
      "[6050] Loss: 0.002301\n",
      "[6100] Loss: 0.003151\n",
      "[6150] Loss: 0.002024\n",
      "[6200] Loss: 0.002561\n",
      "[6250] Loss: 0.003181\n",
      "[6300] Loss: 0.002700\n",
      "[6350] Loss: 0.002940\n",
      "[6400] Loss: 0.002345\n",
      "[6450] Loss: 0.002735\n",
      "[6500] Loss: 0.002408\n",
      "[6550] Loss: 0.002733\n",
      "[6600] Loss: 0.002368\n",
      "[6650] Loss: 0.002272\n",
      "[6700] Loss: 0.002939\n",
      "[6750] Loss: 0.002298\n",
      "[6800] Loss: 0.002958\n",
      "[6850] Loss: 0.002189\n",
      "[6900] Loss: 0.002318\n",
      "[6950] Loss: 0.002896\n",
      "[7000] Loss: 0.002875\n",
      "[7050] Loss: 0.002330\n",
      "[7100] Loss: 0.003069\n",
      "[7150] Loss: 0.003631\n",
      "[7200] Loss: 0.003090\n",
      "[7250] Loss: 0.003382\n",
      "[7300] Loss: 0.002181\n",
      "[7350] Loss: 0.002363\n",
      "[7400] Loss: 0.002797\n",
      "[7450] Loss: 0.002527\n",
      "[7500] Loss: 0.003043\n",
      "[7550] Loss: 0.002474\n",
      "[7600] Loss: 0.002305\n",
      "[7650] Loss: 0.003089\n",
      "[7700] Loss: 0.003247\n",
      "[7750] Loss: 0.002512\n",
      "[7800] Loss: 0.002405\n",
      "[7850] Loss: 0.002542\n",
      "[7900] Loss: 0.002502\n",
      "[7950] Loss: 0.003099\n",
      "[8000] Loss: 0.002672\n",
      "[8050] Loss: 0.002058\n",
      "[8100] Loss: 0.002945\n",
      "[8150] Loss: 0.002267\n",
      "[8200] Loss: 0.002370\n",
      "[8250] Loss: 0.002804\n",
      "[8300] Loss: 0.002850\n",
      "[8350] Loss: 0.002293\n",
      "[8400] Loss: 0.002674\n",
      "[8450] Loss: 0.002407\n",
      "[8500] Loss: 0.002381\n",
      "[8550] Loss: 0.002331\n",
      "[8600] Loss: 0.002681\n",
      "[8650] Loss: 0.002796\n",
      "[8700] Loss: 0.002467\n",
      "[8750] Loss: 0.001683\n",
      "[8800] Loss: 0.002611\n",
      "[8850] Loss: 0.002186\n",
      "[8900] Loss: 0.002150\n",
      "[8950] Loss: 0.002202\n",
      "[9000] Loss: 0.002279\n",
      "[9050] Loss: 0.002229\n",
      "[9100] Loss: 0.002427\n",
      "[9150] Loss: 0.003220\n",
      "[9200] Loss: 0.002297\n",
      "[9250] Loss: 0.002431\n",
      "[9300] Loss: 0.002770\n",
      "[9350] Loss: 0.002909\n",
      "[9400] Loss: 0.002502\n",
      "[9450] Loss: 0.002095\n",
      "[9500] Loss: 0.002515\n",
      "[9550] Loss: 0.002535\n",
      "[9600] Loss: 0.002718\n",
      "[9650] Loss: 0.001963\n",
      "[9700] Loss: 0.002730\n",
      "[9750] Loss: 0.002593\n",
      "[9800] Loss: 0.001701\n",
      "[9850] Loss: 0.002606\n",
      "[9900] Loss: 0.002404\n",
      "[9950] Loss: 0.002538\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Sample noise\n",
    "    x = torch.randn(batch_size, latent_dim, device=device)\n",
    "    \n",
    "    # Sample target latent points\n",
    "    indices = torch.randint(0, latent_tensor.shape[0], (batch_size,))\n",
    "    z = latent_tensor[indices].to(device)\n",
    "    \n",
    "    # Optional time embedding\n",
    "    t = torch.rand(batch_size, 1, device=device)\n",
    "    \n",
    "    # Target vector field: simple difference\n",
    "    u_target = z - x\n",
    "    \n",
    "    # Normalize target\n",
    "    u_mean = u_target.mean(dim=0, keepdim=True)\n",
    "    u_std = u_target.std(dim=0, keepdim=True) + 1e-6\n",
    "    u_target_norm = (u_target - u_mean) / u_std\n",
    "    \n",
    "    # Forward pass\n",
    "    v_pred = vf_model(x, z, t)\n",
    "    \n",
    "    # Loss\n",
    "    loss = F.mse_loss(v_pred, u_target_norm)\n",
    "    \n",
    "    # Backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(vf_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"[{epoch}] Loss: {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d426990-cbc7-46ae-b0b2-d13ed410cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the last u_mean and u_std\n",
    "save_path = data_dir + \"normalization_stats.pt\"\n",
    "torch.save({\n",
    "    'u_mean': u_mean,\n",
    "    'u_std': u_std\n",
    "}, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c1cd184-80d2-48a0-93ac-38e25a43c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = data_dir + \"vf_model_weights.pt\"\n",
    "\n",
    "torch.save(vf_model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b46c5956-d23f-463a-8541-7e85d2e52888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to save the best vector field:\n",
    "class LearnedVectorFieldODE():\n",
    "    def __init__(self, vf_model):\n",
    "        self.vf_model = vf_model\n",
    "\n",
    "    def drift_coefficient(self, x: torch.Tensor, t: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "        # x, z: (batch_size, latent_dim)\n",
    "        # t: (batch_size, 1)\n",
    "        return self.vf_model(x, z, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2376f019-c84c-4f60-bc09-84ec341a1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the trained neural network\n",
    "learned_ode = LearnedVectorFieldODE(vf_model)\n",
    "save_path = data_dir + \"learned_ode.pt\"\n",
    "# Save the wrapper\n",
    "torch.save(learned_ode, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658aae29-b814-4533-bd9b-1f233da81ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
