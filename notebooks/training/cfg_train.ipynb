{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Matching Training\n",
    "\n",
    "**Description:** Training the OT-Flow Matching model with Classifier-Free Guidance. \n",
    "Conditions on both **Cell Type** and **Library Size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import anndata as ad\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "input_file_path = \"/dtu/blackhole/06/213542/paperdata/pbmc3k_train_with_latent.h5ad\"\n",
    "flow_model_save_path = \"/dtu/blackhole/06/213542/paperdata/lib_size_flow_model.pt\"\n",
    "\n",
    "os.makedirs(os.path.dirname(flow_model_save_path), exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 400\n",
    "learning_rate = 5e-4\n",
    "latent_dim = 50\n",
    "p_uncond = 0.1   # Classifier-free guidance dropout probability\n",
    "\n",
    "# Load Data\n",
    "adata = ad.read_h5ad(input_file_path)\n",
    "latent = adata.obsm[\"X_latent\"]\n",
    "latent_tensor = torch.tensor(latent, dtype=torch.float32, device=device)\n",
    "\n",
    "# Library Sizes\n",
    "if \"total_counts\" in adata.obs:\n",
    "    lib_sizes = adata.obs[\"total_counts\"].values\n",
    "else:\n",
    "    lib_sizes = np.array(adata.X.sum(1)).flatten()\n",
    "\n",
    "log_lib_sizes = np.log1p(lib_sizes)\n",
    "log_lib_tensor = torch.tensor(log_lib_sizes, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "# Stats for normalization (needed for training context if we normalize, but mostly for sampling later)\n",
    "lib_min, lib_max = log_lib_tensor.min(), log_lib_tensor.max()\n",
    "lib_mean, lib_std = log_lib_tensor.mean(), log_lib_tensor.std()\n",
    "\n",
    "# Cell Types\n",
    "cell_types = adata.obs[\"cell_type\"].astype(str).values\n",
    "unique_types, inverse_idx = np.unique(cell_types, return_inverse=True)\n",
    "num_cell_types = len(unique_types)\n",
    "cell_type_idx = torch.tensor(inverse_idx, dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Data Shape: {latent.shape}\")\n",
    "print(f\"Library Size: Min={lib_min:.2f}, Max={lib_max:.2f}, Mean={lib_mean:.2f}\")\n",
    "print(f\"Cell Types: {unique_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmpiricalDistribution(nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"data\", data)\n",
    "    def sample(self, n):\n",
    "        idx = torch.randint(0, len(self.data), (n,), device=self.data.device)\n",
    "        return self.data[idx]\n",
    "\n",
    "class GaussianConditionalProbabilityPath:\n",
    "    def __init__(self, p_data):\n",
    "        self.p_data = p_data\n",
    "    def sample_conditional_path(self, z, t):\n",
    "        # Linear interpolation: t * z + (1-t) * noise\n",
    "        return t * z + (1 - t) * torch.randn_like(z)\n",
    "\n",
    "    def conditional_vector_field(self, x_1, x_0):\n",
    "        return x_1 - x_0\n",
    "\n",
    "class TimeEmbedder(nn.Module):\n",
    "    def __init__(self, embed_dim=32):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim), nn.SiLU(),\n",
    "            nn.Linear(embed_dim, embed_dim), nn.SiLU()\n",
    "        )\n",
    "        self.embed_dim = embed_dim\n",
    "    def forward(self, t):\n",
    "        # Sinusoidal embedding\n",
    "        half_dim = self.embed_dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t * emb[None, :]\n",
    "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=1)\n",
    "        return self.mlp(emb)\n",
    "    \n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.mlp(x)\n",
    "\n",
    "class NeuralVectorField(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim=256, n_resblocks=5, time_embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.x_proj = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.c_proj = nn.Linear(latent_dim, hidden_dim) # Condition embedding\n",
    "        self.l_proj = nn.Linear(1, hidden_dim)          # Library Size embedding\n",
    "        self.time_embedder = TimeEmbedder(time_embed_dim)\n",
    "\n",
    "        # Learnable null conditioning vector for CFG\n",
    "        self.null_cond = nn.Parameter(torch.randn(1, latent_dim))\n",
    "\n",
    "        # Input to ResBlocks includes hidden dim * 3 + time\n",
    "        input_dim = hidden_dim * 3 + time_embed_dim \n",
    "        \n",
    "        self.resblocks = nn.ModuleList([\n",
    "            ResNetBlock(input_dim, hidden_dim * 3) for _ in range(n_resblocks)\n",
    "        ])\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3 + time_embed_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t, l):\n",
    "        xh = self.x_proj(x)\n",
    "        ch = self.c_proj(c) \n",
    "        th = self.time_embedder(t)\n",
    "        lh = self.l_proj(l) \n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        h = torch.cat([xh, ch, lh, th], dim=-1)\n",
    "        \n",
    "        for block in self.resblocks:\n",
    "            h = block(h)\n",
    "        return self.output_layer(h)\n",
    "\n",
    "class CellTypeConditioner(nn.Module):\n",
    "    def __init__(self, n_types, latent_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(n_types, latent_dim)\n",
    "    def forward(self, idx):\n",
    "        return self.embed(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "emp_dist = EmpiricalDistribution(latent_tensor)\n",
    "latent_dim = latent_tensor.shape[1]\n",
    "\n",
    "# Initialize models\n",
    "conditioner = CellTypeConditioner(n_types=num_cell_types, latent_dim=latent_dim).to(device)\n",
    "vf_model = NeuralVectorField(latent_dim=latent_dim).to(device)\n",
    "\n",
    "# Optimize both model and conditioner\n",
    "optimizer = torch.optim.AdamW(list(vf_model.parameters()) + list(conditioner.parameters()), lr=learning_rate)\n",
    "\n",
    "epochs_list = []\n",
    "loss_list = []\n",
    "\n",
    "print(\"Starting CFG Training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. Sample Indices and Data\n",
    "    indices = torch.randint(0, latent_tensor.shape[0], (batch_size,))\n",
    "    z = latent_tensor[indices].to(device)          # Target (Data)\n",
    "    c_idx = cell_type_idx[indices].to(device)      # Condition (Cell Type)\n",
    "    l = log_lib_tensor[indices].to(device)         # Condition (Library Size)\n",
    "\n",
    "    # 2. Sample Noise and Time\n",
    "    x = torch.randn(batch_size, latent_dim, device=device)\n",
    "    t = torch.rand(batch_size, 1, device=device)\n",
    "    \n",
    "    # 3. Compute Flow Target (Noise -> Data)\n",
    "    u_target = z - x \n",
    "    \n",
    "    # Normalize target\n",
    "    u_mean = u_target.mean(dim=0, keepdim=True)\n",
    "    u_std = u_target.std(dim=0, keepdim=True) + 1e-6\n",
    "    u_target_norm = (u_target - u_mean) / u_std\n",
    "    \n",
    "    # 4. CFG: Condition masking\n",
    "    c_emb = conditioner(c_idx)\n",
    "    \n",
    "    # Create mask (1 = drop condition, 0 = keep condition)\n",
    "    mask = (torch.rand(batch_size, 1, device=device) < p_uncond).float()\n",
    "    \n",
    "    # Apply Mask: Replace dropped conditions with null_cond\n",
    "    c_input = mask * vf_model.null_cond.expand(batch_size, -1) + (1 - mask) * c_emb\n",
    "    \n",
    "    # 5. Forward pass\n",
    "    v_pred = vf_model(x, c_input, t, l)\n",
    "    \n",
    "    # 6. Loss\n",
    "    loss = F.mse_loss(v_pred, u_target_norm)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(vf_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    epochs_list.append(epoch)\n",
    "    loss_list.append(loss.item())\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"[{epoch}] Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Save Models\n",
    "torch.save({\n",
    "    'vf_state': vf_model.state_dict(),\n",
    "    'cond_state': conditioner.state_dict()\n",
    "}, flow_model_save_path)\n",
    "print(f\"Saved models to {flow_model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}