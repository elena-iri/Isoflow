{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c528d3d-c8c2-414f-a1b7-e4f8fcf28720",
   "metadata": {},
   "source": [
    "Training of the Autoencoder\n",
    "Training on the pbmc3k train dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25798cb-1e66-47fe-a35c-ba694b6090e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/6a/4/214382/venv_2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "\n",
    "\n",
    "import anndata as ad\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Optional, Callable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scvi.distributions import NegativeBinomial\n",
    "import torch.nn.functional as F\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61596c8-9c1e-416c-b822-cd6e02babdfe",
   "metadata": {},
   "source": [
    "Define the function for an MLP to be used in our autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fb4bf0-12d2-40fa-9ae7-251cfe937793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define an MLP as the basis of our encoder and decoder\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dims: List[int],\n",
    "                 batch_norm: bool, \n",
    "                 dropout: bool, \n",
    "                 dropout_p: float, \n",
    "                 activation: Optional[Callable] = nn.ELU, \n",
    "                 final_activation: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = activation\n",
    "        layers = []\n",
    "        for i in range(len(dims[:-2])):\n",
    "            block = [nn.Linear(dims[i], dims[i+1])]\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm1d(dims[i+1]))\n",
    "            block.append(activation())\n",
    "            if dropout:\n",
    "                block.append(nn.Dropout(dropout_p))\n",
    "            layers.append(nn.Sequential(*block))\n",
    "        layers.append(nn.Linear(dims[-2], dims[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        if final_activation == \"tanh\":\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_activation == \"sigmoid\":\n",
    "            self.final_activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.final_activation = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x if self.final_activation is None else self.final_activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f6e3845-2279-488a-aa92-c3e856c29b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "def size_factor_distribution(adata_train, n_samples):\n",
    "    # Parameters\n",
    "    \"\"\" Here we have a function that gets a distribution of the library size in the corresponding data set.\n",
    "    It then samples from this distribution, so we get a realistic distribution of library size in each decoded cell\n",
    "    \"\"\"\n",
    "    min_val = adata_train.obs['n_counts'].min() \n",
    "    max_val = adata_train.obs['n_counts'].max()\n",
    "    mean = adata_train.obs['n_counts'].mean()\n",
    "    std = adata_train.obs['n_counts'].std()\n",
    "    \n",
    "    # Beta distribution sampling\n",
    "    m = (mean - min_val) / (max_val - min_val)\n",
    "    v = (std**2) / ((max_val - min_val)**2)\n",
    "    temp = m*(1-m)/v - 1\n",
    "    a_beta = m * temp\n",
    "    b_beta = (1-m) * temp\n",
    "    \n",
    "    samples_beta = beta.rvs(a_beta, b_beta, size=n_samples)\n",
    "    samples_beta = samples_beta * (max_val - min_val) + min_val\n",
    "\n",
    "    return samples_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c8428-c73d-4578-a081-bb2c8fa9202c",
   "metadata": {},
   "source": [
    "Load the negative binomial autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100f91fe-6b6c-4d95-a5ab-c073eb7dd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we incorporate our MLP into a bigger class, so that we can train an autoencoder. We train it independetly of the \n",
    "# flow matching model.\n",
    "class NB_Autoencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 latent_dim: int = 50,\n",
    "                 hidden_dims: List[int] = [512, 256],\n",
    "                 dropout_p: float = 0.1,\n",
    "                 l2_reg: float = 1e-5,\n",
    "                 kl_reg: float = 0):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.l2_reg = l2_reg\n",
    "        self.kl_reg = kl_reg\n",
    "\n",
    "        self.hidden_encoder = MLP(\n",
    "        dims=[num_features, *hidden_dims, latent_dim],\n",
    "        batch_norm=True,\n",
    "        dropout=False,\n",
    "        dropout_p=dropout_p\n",
    "        )\n",
    "        #self.latent_layer = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        self.decoder = MLP(\n",
    "            dims=[latent_dim, *hidden_dims[::-1], num_features],\n",
    "            batch_norm=True,\n",
    "            dropout=False,\n",
    "            dropout_p=dropout_p\n",
    "        )\n",
    "\n",
    "        #self.log_theta = nn.Parameter(torch.randn(num_features) * 0.01)\n",
    "        self.theta = torch.nn.Parameter(torch.randn(num_features), requires_grad=True)\n",
    "    def forward(self, x, library_size = None):\n",
    "        \"\"\" forward function that encodes and decodes\"\"\"\n",
    "        z = self.hidden_encoder(x[\"X_norm\"])\n",
    "        \n",
    "        #z = self.latent_layer(h)\n",
    "        # Raw decoded logits\n",
    "        logits = self.decoder(z)  \n",
    "        \n",
    "        # Softmax over genes → normalized probabilities\n",
    "        gene_probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        if library_size is None:\n",
    "            # Use average library size 1.0 if not provided\n",
    "            # Sample size factors from your custom distribution\n",
    "            lib = size_factor_distribution(adata, z.size(0))   # returns numpy array or list\n",
    "\n",
    "            # Convert to torch tensor, match shape, move to correct device\n",
    "            library_size = torch.tensor(lib, dtype=torch.float32, device=z.device).unsqueeze(1)\n",
    "\n",
    "            #library_size = torch.ones(z.size(0), 1, device=z.device)\n",
    "        # Library size of each cell (sum of counts)\n",
    "        #library_size = x[\"X\"].sum(1).unsqueeze(1).to(self.device)  \n",
    "        \n",
    "        # Scale probabilities by library size → mean parameter μ\n",
    "        mu = gene_probs * library_size\n",
    " \n",
    "\n",
    "        #theta = torch.exp(self.log_theta).unsqueeze(0).expand_as(mu)\n",
    "        return {\"z\": z, \"mu\": mu, \"theta\": self.theta}\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\" decoding function\"\"\"\n",
    "        z = self.hidden_encoder(x)\n",
    "        return z\n",
    "\n",
    "    def size_factor_distribution(self, adata_train, n_samples):\n",
    "        \"\"\" same function as in block before \"\"\"\n",
    "        min_val = adata_train.obs['n_counts'].min() \n",
    "        max_val = adata_train.obs['n_counts'].max()\n",
    "        mean = adata_train.obs['n_counts'].mean()\n",
    "        std = adata_train.obs['n_counts'].std()\n",
    "        \n",
    "        # Beta distribution sampling\n",
    "        m = (mean - min_val) / (max_val - min_val)\n",
    "        v = (std**2) / ((max_val - min_val)**2)\n",
    "        temp = m*(1-m)/v - 1\n",
    "        a_beta = m * temp\n",
    "        b_beta = (1-m) * temp\n",
    "        \n",
    "        samples_beta = beta.rvs(a_beta, b_beta, size=n_samples)\n",
    "        samples_beta = samples_beta * (max_val - min_val) + min_val\n",
    "    \n",
    "        return samples_beta\n",
    "\n",
    "    \n",
    "        \n",
    "    def decode(self, z, library_size=None):\n",
    "        \"\"\"\n",
    "        Decode latent vectors z to NB parameters mu, theta.\n",
    "        z: (batch, latent_dim)\n",
    "        library_size: (batch, 1) sum of counts per cell; if None, use 1.0\n",
    "        \"\"\"\n",
    "        logits = self.decoder(z)  # (batch, num_genes)\n",
    "        gene_probs = F.softmax(logits, dim=1)  # softmax over genes\n",
    "        # if library size isnt specified:\n",
    "        if library_size is None:\n",
    "            # Sample size factors from your custom distribution\n",
    "            lib = size_factor_distribution(adata, z.size(0))   # returns numpy array or list\n",
    "\n",
    "            # Convert to torch tensor, match shape, move to correct device\n",
    "            library_size = torch.tensor(lib, dtype=torch.float32, device=z.device).unsqueeze(1)\n",
    "\n",
    "            #library_size = torch.ones(z.size(0), 1, device=z.device)\n",
    "    \n",
    "        mu = gene_probs * library_size  # scale by library size\n",
    "        #theta = torch.exp(self.log_theta).unsqueeze(0).expand_as(mu)\n",
    "        return {\"mu\": mu, \"theta\": self.theta}\n",
    "   \n",
    "\n",
    "    def loss_function(self, x, outputs):\n",
    "        \"\"\"\n",
    "        Compute loss using scvi NegativeBinomial.\n",
    "        \"\"\"\n",
    "        mu = outputs[\"mu\"]          # (batch, n_genes)\n",
    "        theta = outputs[\"theta\"]    # (batch, n_genes)\n",
    "        z = outputs[\"z\"]            # latent\n",
    "    \n",
    "        # scvi NegativeBinomial expects mu and theta\n",
    "        nb_dist = NegativeBinomial(mu=mu, theta=torch.exp(self.theta))\n",
    "        nll = -nb_dist.log_prob(x).sum(dim=1).mean()  # mean over batch\n",
    "        \n",
    "        # Optional regularization\n",
    "        l2_loss = sum((p**2).sum() for p in self.parameters()) * self.l2_reg\n",
    "        kl_loss = (z**2).mean() * self.kl_reg\n",
    "    \n",
    "        loss = nll + l2_loss + kl_loss\n",
    "        return {\"loss\": loss, \"nll\": nll}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ab7bf-0a4e-4f1e-b714-1cef66baf45b",
   "metadata": {},
   "source": [
    "Define a dataloader that both gives the raw counts X, and the log1p normalized counts X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93404f22-63dd-4877-8ebe-899a71694207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "class CountsDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        \"\"\"\n",
    "        function that returns a dict containing both the original raw counts and the log1p normalized counts.\n",
    "        X: raw counts tensor (num_cells, num_genes)\n",
    "        y: optional labels tensor (num_cells,)\n",
    "        \"\"\"\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            X = X.toarray()\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.X_norm = torch.log1p(self.X)  # log1p = log(1 + x)\n",
    "        self.y = torch.tensor(y, dtype=torch.long) if y is not None else None\n",
    "        self.n_samples = self.X.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = dict(\n",
    "            X=self.X[idx],\n",
    "            X_norm=self.X_norm[idx]\n",
    "        )\n",
    "        if self.y is not None:\n",
    "            sample[\"y\"] = self.y[idx]\n",
    "        return sample\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c30b2b-0eb8-4e08-babb-7a96232d845d",
   "metadata": {},
   "source": [
    "Load the data and extract raw counts X and check if it can transform correctly with log1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c494f8-6434-4176-96ae-88e35f22bbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2110, 8573])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file_path = \"/dtu/blackhole/0e/214382/datasets/pbmc3k/pbmc3k_train.h5ad\"\n",
    "\n",
    "adata = ad.read_h5ad(input_file_path)\n",
    "adata.obs.head()\n",
    "# extract raw counts\n",
    "X = adata.layers[\"X_counts\"]\n",
    "if hasattr(X, \"toarray\"):\n",
    "    X = X.toarray()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "# check shape is correct\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261bdd0b-5eff-40d7-b0ed-7d9728f2962f",
   "metadata": {},
   "source": [
    "Training loop\n",
    "Very important that the loss is computed on the raw counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46b8731e-23d2-4971-83bf-444e43c47ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_909192/30507050.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000 - Loss: 2765.1971\n",
      "Epoch 100/1000 - Loss: 2611.7810\n",
      "Epoch 150/1000 - Loss: 2474.4409\n",
      "Epoch 200/1000 - Loss: 2337.2923\n",
      "Epoch 250/1000 - Loss: 2212.2679\n",
      "Epoch 300/1000 - Loss: 2103.8249\n",
      "Epoch 350/1000 - Loss: 2022.9652\n",
      "Epoch 400/1000 - Loss: 1963.6307\n",
      "Epoch 450/1000 - Loss: 1901.3625\n",
      "Epoch 500/1000 - Loss: 1873.8554\n",
      "Epoch 550/1000 - Loss: 1839.2425\n",
      "Epoch 600/1000 - Loss: 1823.2363\n",
      "Epoch 650/1000 - Loss: 1791.0843\n",
      "Epoch 700/1000 - Loss: 1791.4225\n",
      "Epoch 750/1000 - Loss: 1770.5874\n",
      "Epoch 800/1000 - Loss: 1756.4842\n",
      "Epoch 850/1000 - Loss: 1742.1504\n",
      "Epoch 900/1000 - Loss: 1736.6346\n",
      "Epoch 950/1000 - Loss: 1723.1017\n",
      "Epoch 1000/1000 - Loss: 1729.5569\n",
      "Trained model saved to /dtu/blackhole/0e/214382/datasets/pbmc3k/pbmc3k_train_nb_autoencoder.pt\n"
     ]
    }
   ],
   "source": [
    "# Main training + encoding\n",
    "\n",
    "input_file = input_file_path\n",
    "latent_dim = 50\n",
    "hidden_dims = [512, 256]\n",
    "batch_size = 512\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "epochs_list=[]\n",
    "loss_list=[] \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#  Load data\n",
    "adata = ad.read_h5ad(input_file)\n",
    "\n",
    "# Load RAW COUNTS\n",
    "X = adata.layers[\"X_counts\"]\n",
    "if hasattr(X, \"toarray\"):\n",
    "    X = X.toarray()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "dataset = CountsDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#  Initialize model \n",
    "num_genes = adata.n_vars\n",
    "model = NB_Autoencoder(num_features=num_genes,\n",
    "                       latent_dim=latent_dim,\n",
    "                       hidden_dims=hidden_dims)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "# we use a adamw model, inspired from similar works\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop \n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # Use log-transformed input for encoder\n",
    "        \n",
    "        batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch)\n",
    "\n",
    "        # Compute loss on raw counts\n",
    "        loss_dict = model.loss_function(batch[\"X\"], outputs)\n",
    "        loss = loss_dict[\"loss\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        #epoch_loss += loss.item() * x_raw.size(0)\n",
    "        epoch_loss += loss.item() * batch[\"X\"].size(0)\n",
    "        \n",
    "\n",
    "    epoch_loss /= len(dataset)\n",
    "    epochs_list.append(epoch)\n",
    "    loss_list.append(epoch_loss)\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save trained model\n",
    "model_file = input_file_path.replace(\".h5ad\", \"_nb_autoencoder.pt\")\n",
    "torch.save(model.state_dict(), model_file)\n",
    "print(f\"Trained model saved to {model_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a70cfe7-29d3-43ef-b775-f10838832bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 58.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent space saved to /dtu/blackhole/0e/214382/datasets/pbmc3k/pbmc3k_train_with_latent.h5ad\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save encoded training cells to train flow model\n",
    "model.eval()\n",
    "all_z = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "        outputs = model.forward(batch)\n",
    "\n",
    "        z = outputs[\"z\"].cpu().numpy()\n",
    "        all_z.append(z)\n",
    "\n",
    "latent = np.concatenate(all_z, axis=0)\n",
    "\n",
    "# Save to AnnData\n",
    "adata.obsm[\"X_latent\"] = latent\n",
    "output_file = input_file.replace(\".h5ad\", \"_with_latent.h5ad\")\n",
    "adata.write(output_file)\n",
    "\n",
    "print(f\"Latent space saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
