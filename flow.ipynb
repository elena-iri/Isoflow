{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53e0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import anndata as ad\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24bf1d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# load the fuckin data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_latent_path = \"/dtu/blackhole/06/213542/adata_preprocessed_subset.h5ad\"\n",
    "flow_model_save_path = \"/dtu/blackhole/06/213542/flow_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load AnnData with latent space ----\n",
    "adata = ad.read_h5ad(train_latent_path)\n",
    "\n",
    "assert \"X_latent\" in adata.obsm, \"Latent representation X_latent not found in .obsm\"\n",
    "z1 = adata.obsm[\"X_latent\"]              # shape [N, 50]\n",
    "\n",
    "# cell type labels\n",
    "assert \"cell_type\" in adata.obs.columns, \"cell_type column not found in adata.obs\"\n",
    "celltypes = pd.Categorical(adata.obs[\"cell_type\"])\n",
    "y_idx = torch.tensor(celltypes.codes, dtype=torch.long)   # integer labels\n",
    "num_celltypes = len(celltypes.categories)\n",
    "\n",
    "# library size (n_counts)\n",
    "assert \"n_counts\" in adata.obs.columns, \"n_counts column not found in adata.obs\"\n",
    "libsize = adata.obs[\"n_counts\"].values.astype(np.float32)\n",
    "log_l = np.log(libsize + 1.0)[:, None]    # shape [N, 1]\n",
    "\n",
    "# ---- Convert to torch tensors ----\n",
    "z1 = torch.tensor(z1, dtype=torch.float32)\n",
    "log_l = torch.tensor(log_l, dtype=torch.float32)\n",
    "\n",
    "# one-hot for cell types\n",
    "y_onehot = F.one_hot(y_idx, num_classes=num_celltypes).float()\n",
    "\n",
    "print(\"z1 shape:\", z1.shape)\n",
    "print(\"y_onehot shape:\", y_onehot.shape)\n",
    "print(\"log_l shape:\", log_l.shape)\n",
    "print(\"Num cell types:\", num_celltypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd80fa-74a6-41cf-925d-47c90c6ccf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFlowDataset(Dataset):\n",
    "    def __init__(self, z, y_onehot, log_l):\n",
    "        assert len(z) == len(y_onehot) == len(log_l)\n",
    "        self.z = z\n",
    "        self.y = y_onehot\n",
    "        self.log_l = log_l\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.z.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.z[idx], self.y[idx], self.log_l[idx]\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataset = LatentFlowDataset(z1, y_onehot, log_l)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "len(dataset), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c950d9-95d1-4caf-ac0a-f16b65860b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFlowDataset(Dataset):\n",
    "    def __init__(self, z, y_onehot, log_l):\n",
    "        assert len(z) == len(y_onehot) == len(log_l)\n",
    "        self.z = z\n",
    "        self.y = y_onehot\n",
    "        self.log_l = log_l\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.z.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.z[idx], self.y[idx], self.log_l[idx]\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataset = LatentFlowDataset(z1, y_onehot, log_l)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "len(dataset), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b65ef-e8e8-4148-91f8-f6914567c83f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VectorField' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#train the flow model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m vf \u001b[38;5;241m=\u001b[39m \u001b[43mVectorField\u001b[49m(latent_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, cond_dim\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(vf\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VectorField' is not defined"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# MLP (copied from autoencoder)\n",
    "# -------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dims: List[int],\n",
    "                 batch_norm: bool = True, \n",
    "                 dropout: bool = True, \n",
    "                 dropout_p: float = 0.1, \n",
    "                 activation: Optional[Callable] = nn.ELU, \n",
    "                 final_activation: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        layers = []\n",
    "        for i in range(len(dims[:-2])):\n",
    "            block = [nn.Linear(dims[i], dims[i+1])]\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm1d(dims[i+1]))\n",
    "            block.append(activation())\n",
    "            if dropout:\n",
    "                block.append(nn.Dropout(dropout_p))\n",
    "            layers.append(nn.Sequential(*block))\n",
    "        layers.append(nn.Linear(dims[-2], dims[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        if final_activation == \"tanh\":\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_activation == \"sigmoid\":\n",
    "            self.final_activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.final_activation = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x if self.final_activation is None else self.final_activation(x)\n",
    "\n",
    "# -------------------------------\n",
    "# Vector Field Network\n",
    "# -------------------------------\n",
    "class VectorField(nn.Module):\n",
    "    def __init__(self, latent_dim: int, cond_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        # time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        # main network\n",
    "        self.net = MLP(\n",
    "            dims=[latent_dim + 64 + cond_dim, hidden_dim, hidden_dim, latent_dim],\n",
    "            batch_norm=True,\n",
    "            dropout=True,\n",
    "            dropout_p=0.1,\n",
    "            activation=nn.ELU,\n",
    "        )\n",
    "\n",
    "    def forward(self, zt, t, cond):\n",
    "        \"\"\"\n",
    "        zt: [B, latent_dim]\n",
    "        t: [B, 1]\n",
    "        cond: [B, cond_dim]\n",
    "        \"\"\"\n",
    "        t_emb = self.time_mlp(t)           # [B, 64]\n",
    "        x = torch.cat([zt, t_emb, cond], dim=1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2f03b-33f0-48cb-9a48-4949c5f06d36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#add CF guidance\u001b[39;00m\n\u001b[1;32m      2\u001b[0m drop_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.15\u001b[39m\n\u001b[0;32m----> 3\u001b[0m mask \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mrand(\u001b[43mB\u001b[49m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m drop_prob)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      5\u001b[0m cond_dropped \u001b[38;5;241m=\u001b[39m cond \u001b[38;5;241m*\u001b[39m mask   \u001b[38;5;66;03m# unconditional samples have cond=0\u001b[39;00m\n\u001b[1;32m      6\u001b[0m pred \u001b[38;5;241m=\u001b[39m vf(zt, t, cond_dropped)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'B' is not defined"
     ]
    }
   ],
   "source": [
    "latent_dim = z1.shape[1]          # 50\n",
    "cond_dim = num_celltypes + 1      # cell types + log size factor\n",
    "\n",
    "vf = VectorField(latent_dim=latent_dim, cond_dim=cond_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(vf.parameters(), lr=1e-4)\n",
    "\n",
    "print(vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66264a-51f6-4d24-a141-8102bd6bf701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "num_epochs = 50             # adjust as you like\n",
    "drop_prob = 0.15            # probability to drop conditioning (for unconditional training)\n",
    "\n",
    "vf.train()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for z1_batch, y_batch, logl_batch in dataloader:\n",
    "        z1_batch = z1_batch.to(device)          # [B, d]\n",
    "        y_batch = y_batch.to(device)            # [B, num_celltypes]\n",
    "        logl_batch = logl_batch.to(device)      # [B, 1]\n",
    "\n",
    "        B, d = z1_batch.shape\n",
    "\n",
    "        # 1) sample noise z0\n",
    "        z0 = torch.randn_like(z1_batch)\n",
    "\n",
    "        # 2) sample time t ~ U(0,1)\n",
    "        t = torch.rand(B, 1, device=device)\n",
    "\n",
    "        # 3) straight-line interpolation\n",
    "        zt = (1.0 - t) * z0 + t * z1_batch\n",
    "\n",
    "        # 4) target velocity u = z1 - z0 (independent of t)\n",
    "        u = z1_batch - z0\n",
    "\n",
    "        # 5) build conditioning vector [onehot, log_l]\n",
    "        cond_full = torch.cat([y_batch, logl_batch], dim=1)   # [B, num_celltypes+1]\n",
    "\n",
    "        # 6) classifier-free guidance training: randomly drop conditioning\n",
    "        mask = (torch.rand(B, 1, device=device) > drop_prob).float()\n",
    "        cond_train = cond_full * mask    # some rows become all zeros (unconditional)\n",
    "\n",
    "        # 7) predicted velocity\n",
    "        pred = vf(zt, t, cond_train)\n",
    "\n",
    "        loss = ((pred - u) ** 2).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / max(1, n_batches)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} - Flow Matching Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# save trained flow model\n",
    "torch.save(vf.state_dict(), flow_model_save_path)\n",
    "print(f\"Flow model saved to {flow_model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
