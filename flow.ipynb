{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53e0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import anndata as ad\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bf1d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# load the fuckin data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_latent_path = \"/dtu/blackhole/06/213542/paperdata/pbmc3k_train_with_latent.h5ad\"\n",
    "flow_model_save_path = \"/dtu/blackhole/06/213542/flow_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cdd1b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1 shape: torch.Size([2110, 50])\n",
      "y_onehot shape: torch.Size([2110, 8])\n",
      "log_l shape: torch.Size([2110, 1])\n",
      "Num cell types: 8\n"
     ]
    }
   ],
   "source": [
    "# ---- Load AnnData with latent space ----\n",
    "adata = ad.read_h5ad(train_latent_path)\n",
    "\n",
    "assert \"X_latent\" in adata.obsm, \"Latent representation X_latent not found in .obsm\"\n",
    "z1 = adata.obsm[\"X_latent\"]              # shape [N, 50]\n",
    "\n",
    "# cell type labels\n",
    "assert \"cell_type\" in adata.obs.columns, \"cell_type column not found in adata.obs\"\n",
    "celltypes = pd.Categorical(adata.obs[\"cell_type\"])\n",
    "y_idx = torch.tensor(celltypes.codes, dtype=torch.long)   # integer labels\n",
    "num_celltypes = len(celltypes.categories)\n",
    "\n",
    "# library size (n_counts)\n",
    "assert \"n_counts\" in adata.obs.columns, \"n_counts column not found in adata.obs\"\n",
    "libsize = adata.obs[\"n_counts\"].values.astype(np.float32)\n",
    "log_l = np.log(libsize + 1.0)[:, None]    # shape [N, 1]\n",
    "\n",
    "# ---- Convert to torch tensors ----\n",
    "z1 = torch.tensor(z1, dtype=torch.float32)\n",
    "log_l = torch.tensor(log_l, dtype=torch.float32)\n",
    "\n",
    "# one-hot for cell types\n",
    "y_onehot = F.one_hot(y_idx, num_classes=num_celltypes).float()\n",
    "\n",
    "print(\"z1 shape:\", z1.shape)\n",
    "print(\"y_onehot shape:\", y_onehot.shape)\n",
    "print(\"log_l shape:\", log_l.shape)\n",
    "print(\"Num cell types:\", num_celltypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3bd80fa-74a6-41cf-925d-47c90c6ccf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2110, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LatentFlowDataset(Dataset):\n",
    "    def __init__(self, z, y_onehot, log_l):\n",
    "        assert len(z) == len(y_onehot) == len(log_l)\n",
    "        self.z = z\n",
    "        self.y = y_onehot\n",
    "        self.log_l = log_l\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.z.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.z[idx], self.y[idx], self.log_l[idx]\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataset = LatentFlowDataset(z1, y_onehot, log_l)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "len(dataset), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c950d9-95d1-4caf-ac0a-f16b65860b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2110, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LatentFlowDataset(Dataset):\n",
    "    def __init__(self, z, y_onehot, log_l):\n",
    "        assert len(z) == len(y_onehot) == len(log_l)\n",
    "        self.z = z\n",
    "        self.y = y_onehot\n",
    "        self.log_l = log_l\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.z.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.z[idx], self.y[idx], self.log_l[idx]\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataset = LatentFlowDataset(z1, y_onehot, log_l)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "len(dataset), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd4b65ef-e8e8-4148-91f8-f6914567c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# MLP (copied from autoencoder)\n",
    "# -------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dims: List[int],\n",
    "                 batch_norm: bool = True, \n",
    "                 dropout: bool = True, \n",
    "                 dropout_p: float = 0.1, \n",
    "                 activation: Optional[Callable] = nn.ELU, \n",
    "                 final_activation: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        layers = []\n",
    "        for i in range(len(dims[:-2])):\n",
    "            block = [nn.Linear(dims[i], dims[i+1])]\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm1d(dims[i+1]))\n",
    "            block.append(activation())\n",
    "            if dropout:\n",
    "                block.append(nn.Dropout(dropout_p))\n",
    "            layers.append(nn.Sequential(*block))\n",
    "        layers.append(nn.Linear(dims[-2], dims[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        if final_activation == \"tanh\":\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_activation == \"sigmoid\":\n",
    "            self.final_activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.final_activation = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x if self.final_activation is None else self.final_activation(x)\n",
    "\n",
    "# -------------------------------\n",
    "# Vector Field Network\n",
    "# -------------------------------\n",
    "class VectorField(nn.Module):\n",
    "    def __init__(self, latent_dim: int, cond_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        # time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        # main network\n",
    "        self.net = MLP(\n",
    "            dims=[latent_dim + 64 + cond_dim, hidden_dim, hidden_dim, latent_dim],\n",
    "            batch_norm=True,\n",
    "            dropout=True,\n",
    "            dropout_p=0.1,\n",
    "            activation=nn.ELU,\n",
    "        )\n",
    "\n",
    "    def forward(self, zt, t, cond):\n",
    "        \"\"\"\n",
    "        zt: [B, latent_dim]\n",
    "        t: [B, 1]\n",
    "        cond: [B, cond_dim]\n",
    "        \"\"\"\n",
    "        t_emb = self.time_mlp(t)           # [B, 64]\n",
    "        x = torch.cat([zt, t_emb, cond], dim=1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f2f03b-33f0-48cb-9a48-4949c5f06d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorField(\n",
      "  (time_mlp): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): SiLU()\n",
      "  )\n",
      "  (net): MLP(\n",
      "    (net): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=123, out_features=256, bias=True)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ELU(alpha=1.0)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ELU(alpha=1.0)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): Linear(in_features=256, out_features=50, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "latent_dim = z1.shape[1]          # 50\n",
    "cond_dim = num_celltypes + 1      # cell types + log size factor\n",
    "\n",
    "vf = VectorField(latent_dim=latent_dim, cond_dim=cond_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(vf.parameters(), lr=1e-4)\n",
    "\n",
    "print(vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac66264a-51f6-4d24-a141-8102bd6bf701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Flow Matching Loss: 3.946148\n",
      "Epoch 2/50 - Flow Matching Loss: 3.670063\n",
      "Epoch 3/50 - Flow Matching Loss: 3.469714\n",
      "Epoch 4/50 - Flow Matching Loss: 3.243708\n",
      "Epoch 5/50 - Flow Matching Loss: 3.082470\n",
      "Epoch 6/50 - Flow Matching Loss: 2.934929\n",
      "Epoch 7/50 - Flow Matching Loss: 2.761802\n",
      "Epoch 8/50 - Flow Matching Loss: 2.688230\n",
      "Epoch 9/50 - Flow Matching Loss: 2.595561\n",
      "Epoch 10/50 - Flow Matching Loss: 2.459846\n",
      "Epoch 11/50 - Flow Matching Loss: 2.386738\n",
      "Epoch 12/50 - Flow Matching Loss: 2.354287\n",
      "Epoch 13/50 - Flow Matching Loss: 2.295142\n",
      "Epoch 14/50 - Flow Matching Loss: 2.204769\n",
      "Epoch 15/50 - Flow Matching Loss: 2.211609\n",
      "Epoch 16/50 - Flow Matching Loss: 2.138831\n",
      "Epoch 17/50 - Flow Matching Loss: 2.070782\n",
      "Epoch 18/50 - Flow Matching Loss: 2.072066\n",
      "Epoch 19/50 - Flow Matching Loss: 2.060832\n",
      "Epoch 20/50 - Flow Matching Loss: 2.031931\n",
      "Epoch 21/50 - Flow Matching Loss: 1.980508\n",
      "Epoch 22/50 - Flow Matching Loss: 1.954944\n",
      "Epoch 23/50 - Flow Matching Loss: 1.927908\n",
      "Epoch 24/50 - Flow Matching Loss: 1.937390\n",
      "Epoch 25/50 - Flow Matching Loss: 1.884414\n",
      "Epoch 26/50 - Flow Matching Loss: 1.848719\n",
      "Epoch 27/50 - Flow Matching Loss: 1.877748\n",
      "Epoch 28/50 - Flow Matching Loss: 1.870323\n",
      "Epoch 29/50 - Flow Matching Loss: 1.845819\n",
      "Epoch 30/50 - Flow Matching Loss: 1.797001\n",
      "Epoch 31/50 - Flow Matching Loss: 1.773183\n",
      "Epoch 32/50 - Flow Matching Loss: 1.790727\n",
      "Epoch 33/50 - Flow Matching Loss: 1.785453\n",
      "Epoch 34/50 - Flow Matching Loss: 1.751674\n",
      "Epoch 35/50 - Flow Matching Loss: 1.713389\n",
      "Epoch 36/50 - Flow Matching Loss: 1.727642\n",
      "Epoch 37/50 - Flow Matching Loss: 1.702010\n",
      "Epoch 38/50 - Flow Matching Loss: 1.689502\n",
      "Epoch 39/50 - Flow Matching Loss: 1.703507\n",
      "Epoch 40/50 - Flow Matching Loss: 1.658913\n",
      "Epoch 41/50 - Flow Matching Loss: 1.647220\n",
      "Epoch 42/50 - Flow Matching Loss: 1.614124\n",
      "Epoch 43/50 - Flow Matching Loss: 1.657816\n",
      "Epoch 44/50 - Flow Matching Loss: 1.601189\n",
      "Epoch 45/50 - Flow Matching Loss: 1.582300\n",
      "Epoch 46/50 - Flow Matching Loss: 1.590696\n",
      "Epoch 47/50 - Flow Matching Loss: 1.577412\n",
      "Epoch 48/50 - Flow Matching Loss: 1.576080\n",
      "Epoch 49/50 - Flow Matching Loss: 1.556831\n",
      "Epoch 50/50 - Flow Matching Loss: 1.556307\n",
      "Flow model saved to /dtu/blackhole/06/213542/flow_model.pt\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "num_epochs = 50             # adjust as you like\n",
    "drop_prob = 0.15            # probability to drop conditioning (for unconditional training)\n",
    "\n",
    "vf.train()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for z1_batch, y_batch, logl_batch in dataloader:\n",
    "        z1_batch = z1_batch.to(device)          # [B, d]\n",
    "        y_batch = y_batch.to(device)            # [B, num_celltypes]\n",
    "        logl_batch = logl_batch.to(device)      # [B, 1]\n",
    "\n",
    "        B, d = z1_batch.shape\n",
    "\n",
    "        # 1) sample noise z0\n",
    "        z0 = torch.randn_like(z1_batch)\n",
    "\n",
    "        # 2) sample time t ~ U(0,1)\n",
    "        t = torch.rand(B, 1, device=device)\n",
    "\n",
    "        # 3) straight-line interpolation\n",
    "        zt = (1.0 - t) * z0 + t * z1_batch\n",
    "\n",
    "        # 4) target velocity u = z1 - z0 (independent of t)\n",
    "        u = z1_batch - z0\n",
    "\n",
    "        # 5) build conditioning vector [onehot, log_l]\n",
    "        cond_full = torch.cat([y_batch, logl_batch], dim=1)   # [B, num_celltypes+1]\n",
    "\n",
    "        # 6) classifier-free guidance training: randomly drop conditioning\n",
    "        mask = (torch.rand(B, 1, device=device) > drop_prob).float()\n",
    "        cond_train = cond_full * mask    # some rows become all zeros (unconditional)\n",
    "\n",
    "        # 7) predicted velocity\n",
    "        pred = vf(zt, t, cond_train)\n",
    "\n",
    "        loss = ((pred - u) ** 2).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / max(1, n_batches)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} - Flow Matching Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# save trained flow model\n",
    "torch.save(vf.state_dict(), flow_model_save_path)\n",
    "print(f\"Flow model saved to {flow_model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c4b0f-746c-441e-86d1-af63998fb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- Utility: build conditioning batches ---\n",
    "\n",
    "def build_cond_batch(celltype_idx: int,\n",
    "                     log_l_value: float,\n",
    "                     n_samples: int,\n",
    "                     num_celltypes: int,\n",
    "                     device: torch.device):\n",
    "    \"\"\"\n",
    "    Build conditioning matrix [onehot(celltype), log_l] for n_samples.\n",
    "    \"\"\"\n",
    "    y_idx = torch.full((n_samples,), celltype_idx, dtype=torch.long, device=device)\n",
    "    y_onehot = F.one_hot(y_idx, num_classes=num_celltypes).float()        # [B, num_celltypes]\n",
    "    log_l = torch.full((n_samples, 1), float(log_l_value), device=device) # [B, 1]\n",
    "    return torch.cat([y_onehot, log_l], dim=1)                            # [B, num_celltypes+1]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_latent_from_flow(\n",
    "    vf: VectorField,\n",
    "    cond: torch.Tensor,\n",
    "    latent_dim: int,\n",
    "    n_steps: int = 100,\n",
    "    guidance_scale: float = 1.5,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Integrate dz/dt = v_theta(z, t, cond) from t=0 to 1 with Euler steps.\n",
    "\n",
    "    cond: [B, cond_dim] (same for all steps)\n",
    "    returns: z1_samples [B, latent_dim]\n",
    "    \"\"\"\n",
    "    vf.eval()\n",
    "\n",
    "    B = cond.shape[0]\n",
    "    cond = cond.to(device)\n",
    "\n",
    "    # initial noise (t=0)\n",
    "    z = torch.randn(B, latent_dim, device=device)\n",
    "\n",
    "    dt = 1.0 / n_steps\n",
    "    t = torch.zeros(B, 1, device=device)\n",
    "\n",
    "    for k in range(n_steps):\n",
    "        t.fill_(k * dt)\n",
    "\n",
    "        # classifier-free guidance at sampling:\n",
    "        # v = v_uncond + guidance_scale * (v_cond - v_uncond)\n",
    "        v_uncond = vf(z, t, torch.zeros_like(cond))\n",
    "        v_cond   = vf(z, t, cond)\n",
    "        v = v_uncond + guidance_scale * (v_cond - v_uncond)\n",
    "\n",
    "        z = z + v * dt\n",
    "\n",
    "    return z.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505fc815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cell type 'CD4 T cells' (idx=0) with median log_l=7.747\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_cond_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing cell type \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_celltype_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (idx=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcelltype_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith median log_l=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_l_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Build conditioning batch and sample\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m cond_batch = \u001b[43mbuild_cond_batch\u001b[49m(\n\u001b[32m     20\u001b[39m     celltype_idx=celltype_idx,\n\u001b[32m     21\u001b[39m     log_l_value=log_l_value,\n\u001b[32m     22\u001b[39m     n_samples=n_samples,\n\u001b[32m     23\u001b[39m     num_celltypes=num_celltypes,\n\u001b[32m     24\u001b[39m     device=device,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m z1_samples = sample_latent_from_flow(\n\u001b[32m     28\u001b[39m     vf=vf,\n\u001b[32m     29\u001b[39m     cond=cond_batch,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     device=device,\n\u001b[32m     34\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerated latent samples:\u001b[39m\u001b[33m\"\u001b[39m, z1_samples.shape)  \u001b[38;5;66;03m# [1000, latent_dim]\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'build_cond_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Example: generate 1000 cells of a chosen cell type\n",
    "target_celltype_name = \"CD4 T cells\" \n",
    "n_samples = 1000\n",
    "\n",
    "# Map name -> index in your existing 'celltypes' categorical\n",
    "categories = list(celltypes.categories)\n",
    "assert target_celltype_name in categories, f\"{target_celltype_name} not in {categories}\"\n",
    "celltype_idx = categories.index(target_celltype_name)\n",
    "\n",
    "# Get median log library size for that cell type from training data\n",
    "mask_ct = (celltypes == target_celltype_name)\n",
    "log_l_ct = np.log(adata.obs.loc[mask_ct, \"n_counts\"].values.astype(np.float32) + 1.0)\n",
    "log_l_value = float(np.median(log_l_ct))\n",
    "\n",
    "print(f\"Using cell type '{target_celltype_name}' (idx={celltype_idx}) \"\n",
    "      f\"with median log_l={log_l_value:.3f}\")\n",
    "\n",
    "# Build conditioning batch and sample\n",
    "cond_batch = build_cond_batch(\n",
    "    celltype_idx=celltype_idx,\n",
    "    log_l_value=log_l_value,\n",
    "    n_samples=n_samples,\n",
    "    num_celltypes=num_celltypes,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "z1_samples = sample_latent_from_flow(\n",
    "    vf=vf,\n",
    "    cond=cond_batch,\n",
    "    latent_dim=latent_dim,\n",
    "    n_steps=100,          # you can tune\n",
    "    guidance_scale=1.5,   # you can tune (>=1)\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Generated latent samples:\", z1_samples.shape)  # [1000, latent_dim]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
