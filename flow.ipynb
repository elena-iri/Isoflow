{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53e0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import anndata as ad\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24bf1d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# load the fuckin data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_latent_path = \"/dtu/blackhole/06/213542/paperdata/pbmc3k_train_with_latent.h5ad\"\n",
    "flow_model_save_path = \"/dtu/blackhole/06/213542/flow_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cdd1b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1 shape: torch.Size([2110, 50])\n",
      "y_onehot shape: torch.Size([2110, 8])\n",
      "log_l shape: torch.Size([2110, 1])\n",
      "Num cell types: 8\n"
     ]
    }
   ],
   "source": [
    "# ---- Load AnnData with latent space ----\n",
    "adata = ad.read_h5ad(train_latent_path)\n",
    "\n",
    "assert \"X_latent\" in adata.obsm, \"Latent representation X_latent not found in .obsm\"\n",
    "z1 = adata.obsm[\"X_latent\"]              # shape [N, 50]\n",
    "\n",
    "# cell type labels\n",
    "assert \"cell_type\" in adata.obs.columns, \"cell_type column not found in adata.obs\"\n",
    "celltypes = pd.Categorical(adata.obs[\"cell_type\"])\n",
    "y_idx = torch.tensor(celltypes.codes, dtype=torch.long)   # integer labels\n",
    "num_celltypes = len(celltypes.categories)\n",
    "\n",
    "# library size (n_counts)\n",
    "assert \"n_counts\" in adata.obs.columns, \"n_counts column not found in adata.obs\"\n",
    "libsize = adata.obs[\"n_counts\"].values.astype(np.float32)\n",
    "log_l = np.log(libsize + 1.0)[:, None]    # shape [N, 1]\n",
    "\n",
    "# ---- Convert to torch tensors ----\n",
    "z1 = torch.tensor(z1, dtype=torch.float32)\n",
    "log_l = torch.tensor(log_l, dtype=torch.float32)\n",
    "\n",
    "# one-hot for cell types\n",
    "y_onehot = F.one_hot(y_idx, num_classes=num_celltypes).float()\n",
    "\n",
    "print(\"z1 shape:\", z1.shape)\n",
    "print(\"y_onehot shape:\", y_onehot.shape)\n",
    "print(\"log_l shape:\", log_l.shape)\n",
    "print(\"Num cell types:\", num_celltypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3bd80fa-74a6-41cf-925d-47c90c6ccf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2110, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LatentFlowDataset(Dataset):\n",
    "    def __init__(self, z, y_onehot, log_l):\n",
    "        assert len(z) == len(y_onehot) == len(log_l)\n",
    "        self.z = z\n",
    "        self.y = y_onehot\n",
    "        self.log_l = log_l\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.z.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.z[idx], self.y[idx], self.log_l[idx]\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataset = LatentFlowDataset(z1, y_onehot, log_l)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "len(dataset), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c950d9-95d1-4caf-ac0a-f16b65860b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2110, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LatentFlowDataset(Dataset):\n",
    "    def __init__(self, z, y_onehot, log_l):\n",
    "        assert len(z) == len(y_onehot) == len(log_l)\n",
    "        self.z = z\n",
    "        self.y = y_onehot\n",
    "        self.log_l = log_l\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.z.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.z[idx], self.y[idx], self.log_l[idx]\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataset = LatentFlowDataset(z1, y_onehot, log_l)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "len(dataset), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4b65ef-e8e8-4148-91f8-f6914567c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# MLP (copied from autoencoder)\n",
    "# -------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dims: List[int],\n",
    "                 batch_norm: bool = True, \n",
    "                 dropout: bool = True, \n",
    "                 dropout_p: float = 0.1, \n",
    "                 activation: Optional[Callable] = nn.ELU, \n",
    "                 final_activation: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        layers = []\n",
    "        for i in range(len(dims[:-2])):\n",
    "            block = [nn.Linear(dims[i], dims[i+1])]\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm1d(dims[i+1]))\n",
    "            block.append(activation())\n",
    "            if dropout:\n",
    "                block.append(nn.Dropout(dropout_p))\n",
    "            layers.append(nn.Sequential(*block))\n",
    "        layers.append(nn.Linear(dims[-2], dims[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        if final_activation == \"tanh\":\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_activation == \"sigmoid\":\n",
    "            self.final_activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.final_activation = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x if self.final_activation is None else self.final_activation(x)\n",
    "\n",
    "# -------------------------------\n",
    "# Vector Field Network\n",
    "# -------------------------------\n",
    "class VectorField(nn.Module):\n",
    "    def __init__(self, latent_dim: int, cond_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        # time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        # main network\n",
    "        self.net = MLP(\n",
    "            dims=[latent_dim + 64 + cond_dim, hidden_dim, hidden_dim, latent_dim],\n",
    "            batch_norm=True,\n",
    "            dropout=True,\n",
    "            dropout_p=0.1,\n",
    "            activation=nn.ELU,\n",
    "        )\n",
    "\n",
    "    def forward(self, zt, t, cond):\n",
    "        \"\"\"\n",
    "        zt: [B, latent_dim]\n",
    "        t: [B, 1]\n",
    "        cond: [B, cond_dim]\n",
    "        \"\"\"\n",
    "        t_emb = self.time_mlp(t)           # [B, 64]\n",
    "        x = torch.cat([zt, t_emb, cond], dim=1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99f2f03b-33f0-48cb-9a48-4949c5f06d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorField(\n",
      "  (time_mlp): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): SiLU()\n",
      "  )\n",
      "  (net): MLP(\n",
      "    (net): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=123, out_features=256, bias=True)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ELU(alpha=1.0)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ELU(alpha=1.0)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): Linear(in_features=256, out_features=50, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "latent_dim = z1.shape[1]          # 50\n",
    "cond_dim = num_celltypes + 1      # cell types + log size factor\n",
    "\n",
    "vf = VectorField(latent_dim=latent_dim, cond_dim=cond_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(vf.parameters(), lr=1e-4)\n",
    "\n",
    "print(vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac66264a-51f6-4d24-a141-8102bd6bf701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Flow Matching Loss: 4.035111\n",
      "Epoch 2/50 - Flow Matching Loss: 3.776642\n",
      "Epoch 3/50 - Flow Matching Loss: 3.524265\n",
      "Epoch 4/50 - Flow Matching Loss: 3.351206\n",
      "Epoch 5/50 - Flow Matching Loss: 3.152337\n",
      "Epoch 6/50 - Flow Matching Loss: 2.972851\n",
      "Epoch 7/50 - Flow Matching Loss: 2.869794\n",
      "Epoch 8/50 - Flow Matching Loss: 2.758221\n",
      "Epoch 9/50 - Flow Matching Loss: 2.672962\n",
      "Epoch 10/50 - Flow Matching Loss: 2.512293\n",
      "Epoch 11/50 - Flow Matching Loss: 2.509797\n",
      "Epoch 12/50 - Flow Matching Loss: 2.392600\n",
      "Epoch 13/50 - Flow Matching Loss: 2.321366\n",
      "Epoch 14/50 - Flow Matching Loss: 2.257937\n",
      "Epoch 15/50 - Flow Matching Loss: 2.217716\n",
      "Epoch 16/50 - Flow Matching Loss: 2.145282\n",
      "Epoch 17/50 - Flow Matching Loss: 2.151288\n",
      "Epoch 18/50 - Flow Matching Loss: 2.111206\n",
      "Epoch 19/50 - Flow Matching Loss: 2.083393\n",
      "Epoch 20/50 - Flow Matching Loss: 2.085620\n",
      "Epoch 21/50 - Flow Matching Loss: 2.036394\n",
      "Epoch 22/50 - Flow Matching Loss: 2.000442\n",
      "Epoch 23/50 - Flow Matching Loss: 1.947430\n",
      "Epoch 24/50 - Flow Matching Loss: 1.958070\n",
      "Epoch 25/50 - Flow Matching Loss: 1.914727\n",
      "Epoch 26/50 - Flow Matching Loss: 1.912288\n",
      "Epoch 27/50 - Flow Matching Loss: 1.913424\n",
      "Epoch 28/50 - Flow Matching Loss: 1.875258\n",
      "Epoch 29/50 - Flow Matching Loss: 1.824624\n",
      "Epoch 30/50 - Flow Matching Loss: 1.847967\n",
      "Epoch 31/50 - Flow Matching Loss: 1.830971\n",
      "Epoch 32/50 - Flow Matching Loss: 1.791399\n",
      "Epoch 33/50 - Flow Matching Loss: 1.834410\n",
      "Epoch 34/50 - Flow Matching Loss: 1.792129\n",
      "Epoch 35/50 - Flow Matching Loss: 1.750544\n",
      "Epoch 36/50 - Flow Matching Loss: 1.759755\n",
      "Epoch 37/50 - Flow Matching Loss: 1.752548\n",
      "Epoch 38/50 - Flow Matching Loss: 1.695677\n",
      "Epoch 39/50 - Flow Matching Loss: 1.709565\n",
      "Epoch 40/50 - Flow Matching Loss: 1.728267\n",
      "Epoch 41/50 - Flow Matching Loss: 1.670092\n",
      "Epoch 42/50 - Flow Matching Loss: 1.679656\n",
      "Epoch 43/50 - Flow Matching Loss: 1.663022\n",
      "Epoch 44/50 - Flow Matching Loss: 1.613267\n",
      "Epoch 45/50 - Flow Matching Loss: 1.612765\n",
      "Epoch 46/50 - Flow Matching Loss: 1.610726\n",
      "Epoch 47/50 - Flow Matching Loss: 1.602542\n",
      "Epoch 48/50 - Flow Matching Loss: 1.631973\n",
      "Epoch 49/50 - Flow Matching Loss: 1.581097\n",
      "Epoch 50/50 - Flow Matching Loss: 1.580825\n",
      "Flow model saved to /dtu/blackhole/06/213542/flow_model.pt\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "num_epochs = 50             # adjust as you like\n",
    "drop_prob = 0.15            # probability to drop conditioning (for unconditional training)\n",
    "\n",
    "vf.train()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for z1_batch, y_batch, logl_batch in dataloader:\n",
    "        z1_batch = z1_batch.to(device)          # [B, d]\n",
    "        y_batch = y_batch.to(device)            # [B, num_celltypes]\n",
    "        logl_batch = logl_batch.to(device)      # [B, 1]\n",
    "\n",
    "        B, d = z1_batch.shape\n",
    "\n",
    "        # 1) sample noise z0\n",
    "        z0 = torch.randn_like(z1_batch)\n",
    "\n",
    "        # 2) sample time t ~ U(0,1)\n",
    "        t = torch.rand(B, 1, device=device)\n",
    "\n",
    "        # 3) straight-line interpolation\n",
    "        zt = (1.0 - t) * z0 + t * z1_batch\n",
    "\n",
    "        # 4) target velocity u = z1 - z0 (independent of t)\n",
    "        u = z1_batch - z0\n",
    "\n",
    "        # 5) build conditioning vector [onehot, log_l]\n",
    "        cond_full = torch.cat([y_batch, logl_batch], dim=1)   # [B, num_celltypes+1]\n",
    "\n",
    "        # 6) classifier-free guidance training: randomly drop conditioning\n",
    "        mask = (torch.rand(B, 1, device=device) > drop_prob).float()\n",
    "        cond_train = cond_full * mask    # some rows become all zeros (unconditional)\n",
    "\n",
    "        # 7) predicted velocity\n",
    "        pred = vf(zt, t, cond_train)\n",
    "\n",
    "        loss = ((pred - u) ** 2).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / max(1, n_batches)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} - Flow Matching Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# save trained flow model\n",
    "torch.save(vf.state_dict(), flow_model_save_path)\n",
    "print(f\"Flow model saved to {flow_model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c4b0f-746c-441e-86d1-af63998fb453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
