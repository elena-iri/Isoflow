{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22e427f",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f6efc0",
   "metadata": {},
   "source": [
    "We inspired our encoder in the one used in CFGEN, which has is a multi-layer perceptron (MLP) with two hidden layers of dimensions\n",
    "[512, 256] that map the input to a 50-dimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622c038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from typing import List, Optional, Callable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c7d4a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Define MLP (like the one in CFGEN)\n",
    "# -------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dims: List[int],\n",
    "                 batch_norm: bool = True, \n",
    "                 dropout: bool = True, \n",
    "                 dropout_p: float = 0.1, \n",
    "                 activation: Optional[Callable] = nn.ELU, \n",
    "                 final_activation: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        layers = []\n",
    "        for i in range(len(dims[:-2])):\n",
    "            block = [nn.Linear(dims[i], dims[i+1])]\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm1d(dims[i+1]))\n",
    "            block.append(activation())\n",
    "            if dropout:\n",
    "                block.append(nn.Dropout(dropout_p))\n",
    "            layers.append(nn.Sequential(*block))\n",
    "        layers.append(nn.Linear(dims[-2], dims[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        if final_activation == \"tanh\":\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_activation == \"sigmoid\":\n",
    "            self.final_activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.final_activation = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x if self.final_activation is None else self.final_activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb3bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Negative Binomial log-likelihood\n",
    "# -------------------------------\n",
    "def negative_binomial_log_likelihood(x, mu, theta, eps=1e-8):\n",
    "    t1 = torch.lgamma(theta + eps) + torch.lgamma(x + 1.0) - torch.lgamma(x + theta + eps)\n",
    "    t2 = (theta * (torch.log(theta + eps) - torch.log(mu + theta + eps))) + \\\n",
    "         (x * (torch.log(mu + eps) - torch.log(mu + theta + eps)))\n",
    "    return t1 + t2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5233f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# NB Autoencoder\n",
    "# -------------------------------\n",
    "class NB_Autoencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 latent_dim: int = 50,\n",
    "                 hidden_dims: List[int] = [512, 256],\n",
    "                 dropout_p: float = 0.1,\n",
    "                 l2_reg: float = 1e-5,\n",
    "                 kl_reg: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.l2_reg = l2_reg\n",
    "        self.kl_reg = kl_reg\n",
    "\n",
    "        self.encoder = MLP(\n",
    "            dims=[num_features, *hidden_dims, latent_dim],\n",
    "            batch_norm=True,\n",
    "            dropout=True,\n",
    "            dropout_p=dropout_p\n",
    "        )\n",
    "\n",
    "        self.decoder = MLP(\n",
    "            dims=[latent_dim, *hidden_dims[::-1], num_features],\n",
    "            batch_norm=True,\n",
    "            dropout=True,\n",
    "            dropout_p=dropout_p\n",
    "        )\n",
    "\n",
    "        self.log_theta = nn.Parameter(torch.randn(num_features) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        mu = F.softplus(self.decoder(z))\n",
    "        theta = torch.exp(self.log_theta).unsqueeze(0).expand_as(mu)\n",
    "        return {\"z\": z, \"mu\": mu, \"theta\": theta}\n",
    "\n",
    "    def loss_function(self, x, outputs):\n",
    "        mu = outputs[\"mu\"]\n",
    "        theta = outputs[\"theta\"]\n",
    "        z = outputs[\"z\"]\n",
    "        nll = -negative_binomial_log_likelihood(x, mu, theta).sum(dim=1).mean()\n",
    "        l2_loss = sum((p**2).sum() for p in self.parameters()) * self.l2_reg\n",
    "        kl_loss = (z**2).mean() * self.kl_reg\n",
    "        loss = nll + l2_loss + kl_loss\n",
    "        return {\"loss\": loss, \"nll\": nll, \"l2\": l2_loss, \"kl\": kl_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b4d9b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_genes</th>\n",
       "      <th>percent_mito</th>\n",
       "      <th>n_counts</th>\n",
       "      <th>cell_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CCAATTTGAACGTC-1</th>\n",
       "      <td>932</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>2671.0</td>\n",
       "      <td>CD14+ Monocytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AACAGCACAAGAGT-1</th>\n",
       "      <td>343</td>\n",
       "      <td>0.024709</td>\n",
       "      <td>688.0</td>\n",
       "      <td>CD14+ Monocytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGAGATGACTGAAC-1</th>\n",
       "      <td>678</td>\n",
       "      <td>0.023834</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>CD4 T cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AATAGGGAGAATGA-1</th>\n",
       "      <td>756</td>\n",
       "      <td>0.036002</td>\n",
       "      <td>1861.0</td>\n",
       "      <td>CD8 T cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCCTGACTCTCAAG-1</th>\n",
       "      <td>757</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>CD14+ Monocytes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  n_genes  percent_mito  n_counts        cell_type\n",
       "index                                                             \n",
       "CCAATTTGAACGTC-1      932      0.010483    2671.0  CD14+ Monocytes\n",
       "AACAGCACAAGAGT-1      343      0.024709     688.0  CD14+ Monocytes\n",
       "AGAGATGACTGAAC-1      678      0.023834    1972.0      CD4 T cells\n",
       "AATAGGGAGAATGA-1      756      0.036002    1861.0      CD8 T cells\n",
       "GCCTGACTCTCAAG-1      757      0.014493    2001.0  CD14+ Monocytes"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if it's corrupted\n",
    "import anndata as ad\n",
    "input_file_path = \"/dtu/blackhole/06/213542/paperdata/pbmc3k_train.h5ad\"\n",
    "#adata = ad.read_h5ad(\"/zhome/99/9/213566/deepL/adata_preprocessed_subset.h5ad\", backed='r')\n",
    "adata = ad.read_h5ad(input_file_path, backed='r')\n",
    "adata.obs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1d35750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 5834.202, NLL: 5413.974\n",
      "Epoch 2/5, Loss: 4930.286, NLL: 4476.204\n",
      "Epoch 3/5, Loss: 4199.324, NLL: 3906.454\n",
      "Epoch 4/5, Loss: 3704.829, NLL: 3630.023\n",
      "Epoch 5/5, Loss: 3445.340, NLL: 3424.888\n",
      "Trained model saved to /dtu/blackhole/1e/213566/datasets/pbmc3k/pbmc3k_train_nb_autoencoder.pt\n",
      "Latent space saved to /dtu/blackhole/1e/213566/datasets/pbmc3k/pbmc3k_train_with_latent.h5ad\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Main training + encoding\n",
    "# -------------------------------\n",
    "    # --- Hyperparameters ---\n",
    "input_file = input_file_path\n",
    "latent_dim = 50\n",
    "hidden_dims = [512, 256]\n",
    "batch_size = 512\n",
    "epochs = 5            # short run to check\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- Load data ---\n",
    "adata = anndata.read_h5ad(input_file)\n",
    "X = adata.X\n",
    "if hasattr(X, \"toarray\"):\n",
    "    X = X.toarray()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # --- Initialize model ---\n",
    "num_genes = adata.n_vars\n",
    "model = NB_Autoencoder(num_features=num_genes, latent_dim=latent_dim, hidden_dims=hidden_dims)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # --- Training loop ---\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_batch = batch[0].to(device)\n",
    "        outputs = model(x_batch)\n",
    "        loss_dict = model.loss_function(x_batch, outputs)\n",
    "        loss = loss_dict[\"loss\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    epoch_loss /= len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.3f}, NLL: {loss_dict['nll'].item():.3f}\")\n",
    "\n",
    "    # --- Save trained model ---\n",
    "model_file = input_file_path.replace(\".h5ad\", \"_nb_autoencoder.pt\")\n",
    "torch.save(model.state_dict(), model_file)\n",
    "print(f\"Trained model saved to {model_file}\")\n",
    "\n",
    "    # --- Encode all cells into latent space ---\n",
    "model.eval()\n",
    "all_z = []\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(dataset, batch_size=batch_size):\n",
    "        x_batch = batch[0].to(device)\n",
    "        z = model(x_batch)[\"z\"].cpu().numpy()\n",
    "        all_z.append(z)\n",
    "latent = np.concatenate(all_z, axis=0)\n",
    "\n",
    "    # --- Save latent space to AnnData ---\n",
    "adata.obsm[\"X_latent\"] = latent\n",
    "output_file = input_file.replace(\".h5ad\", \"_with_latent.h5ad\")\n",
    "adata.write(output_file)\n",
    "print(f\"Latent space saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6e980",
   "metadata": {},
   "source": [
    "Running on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27f7366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path2 = \"/dtu/blackhole/1e/213566/datasets/pbmc3k/pbmc3k_test.h5ad\"\n",
    "new_adata = anndata.read_h5ad(input_file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0466e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load model ---\n",
    "num_genes = new_adata.n_vars\n",
    "model = NB_Autoencoder(num_features=num_genes, latent_dim=50, hidden_dims=[512, 256])\n",
    "model.load_state_dict(torch.load(\"/dtu/blackhole/1e/213566/datasets/pbmc3k/pbmc3k_train_nb_autoencoder.pt\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Encode new cells\n",
    "X_new = new_adata.X\n",
    "if hasattr(X_new, \"toarray\"):\n",
    "    X_new = X_new.toarray()\n",
    "X_new = torch.tensor(X_new, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_new = model(X_new)[\"z\"].cpu().numpy()\n",
    "\n",
    "# Save to AnnData\n",
    "new_adata.obsm[\"X_latent\"] = z_new\n",
    "new_adata.write(\"/dtu/blackhole/1e/213566/datasets/pbmc3k/new_cells_with_latent.h5ad\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
