{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22e427f",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f6efc0",
   "metadata": {},
   "source": [
    "We inspired our encoder in the one used in CFGEN, which has is a multi-layer perceptron (MLP) with two hidden layers of dimensions\n",
    "[512, 256] that map the input to a 50-dimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "622c038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder notebook but not for training just for using to encode and decode\n",
    "\n",
    "import anndata as ad\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from typing import List, Optional, Callable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c7d4a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Define MLP (like the one in CFGEN)\n",
    "# -------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dims: List[int],\n",
    "                 batch_norm: bool = True, \n",
    "                 dropout: bool = True, \n",
    "                 dropout_p: float = 0.1, \n",
    "                 activation: Optional[Callable] = nn.ELU, \n",
    "                 final_activation: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        layers = []\n",
    "        for i in range(len(dims[:-2])):\n",
    "            block = [nn.Linear(dims[i], dims[i+1])]\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm1d(dims[i+1]))\n",
    "            block.append(activation())\n",
    "            if dropout:\n",
    "                block.append(nn.Dropout(dropout_p))\n",
    "            layers.append(nn.Sequential(*block))\n",
    "        layers.append(nn.Linear(dims[-2], dims[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        if final_activation == \"tanh\":\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_activation == \"sigmoid\":\n",
    "            self.final_activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.final_activation = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x if self.final_activation is None else self.final_activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bb3bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Negative Binomial log-likelihood\n",
    "# -------------------------------\n",
    "def negative_binomial_log_likelihood(x, mu, theta, eps=1e-8):\n",
    "    t1 = torch.lgamma(theta + eps) + torch.lgamma(x + 1.0) - torch.lgamma(x + theta + eps)\n",
    "    t2 = (theta * (torch.log(theta + eps) - torch.log(mu + theta + eps))) + \\\n",
    "         (x * (torch.log(mu + eps) - torch.log(mu + theta + eps)))\n",
    "    return t1 + t2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5233f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# NB Autoencoder\n",
    "# -------------------------------\n",
    "class NB_Autoencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 latent_dim: int = 50,\n",
    "                 hidden_dims: List[int] = [512, 256],\n",
    "                 dropout_p: float = 0.1,\n",
    "                 l2_reg: float = 1e-5,\n",
    "                 kl_reg: float = 0):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.l2_reg = l2_reg\n",
    "        self.kl_reg = kl_reg\n",
    "\n",
    "        self.hidden_encoder = MLP(\n",
    "        dims=[num_features, *hidden_dims],\n",
    "        batch_norm=True,\n",
    "        dropout=True,\n",
    "        dropout_p=dropout_p\n",
    "        )\n",
    "        self.latent_layer = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "\n",
    "        self.decoder = MLP(\n",
    "            dims=[latent_dim, *hidden_dims[::-1], num_features],\n",
    "            batch_norm=True,\n",
    "            dropout=True,\n",
    "            dropout_p=dropout_p\n",
    "        )\n",
    "\n",
    "        self.log_theta = nn.Parameter(torch.randn(num_features) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.hidden_encoder(x)\n",
    "        z = self.latent_layer(h)\n",
    "        # Raw decoded logits\n",
    "        logits = self.decoder(z)   # shape (batch, num_genes)\n",
    "        \n",
    "        # Softmax over genes → normalized probabilities\n",
    "        gene_probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Library size of each cell (sum of counts)\n",
    "        library_size = x.sum(dim=1, keepdim=True)  # shape (batch, 1)\n",
    "        \n",
    "        # Scale probabilities by library size → mean parameter μ\n",
    "        mu = gene_probs * library_size\n",
    "\n",
    "        theta = torch.exp(self.log_theta).unsqueeze(0).expand_as(mu)\n",
    "        return {\"z\": z, \"mu\": mu, \"theta\": theta}\n",
    "\n",
    "    def decode(self, z):\n",
    "        mu = F.softplus(self.decoder(z))\n",
    "        theta = torch.exp(self.log_theta).unsqueeze(0).expand_as(mu)\n",
    "        return {\"mu\": mu, \"theta\": theta}\n",
    "\n",
    "    def loss_function(self, x, outputs):\n",
    "        mu = outputs[\"mu\"]\n",
    "        theta = outputs[\"theta\"]\n",
    "        z = outputs[\"z\"]\n",
    "        nll = -negative_binomial_log_likelihood(x, mu, theta).sum(dim=1).mean()\n",
    "        l2_loss = sum((p**2).sum() for p in self.parameters()) * self.l2_reg\n",
    "        kl_loss = (z**2).mean() * self.kl_reg\n",
    "        loss = nll + l2_loss + kl_loss\n",
    "        return {\"loss\": loss, \"nll\": nll, \"l2\": l2_loss, \"kl\": kl_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b4d9b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2110, 8573])\n"
     ]
    }
   ],
   "source": [
    "#check if it's corrupted\n",
    "import anndata as ad\n",
    "input_file_path = \"/dtu/blackhole/0e/214382/datasets/pbmc3k/pbmc3k_train.h5ad\"\n",
    "#adata = ad.read_h5ad(\"/zhome/99/9/213566/deepL/adata_preprocessed_subset.h5ad\", backed='r')\n",
    "adata = ad.read_h5ad(input_file_path)\n",
    "adata.obs.head()\n",
    "X = adata.layers[\"X_counts\"]\n",
    "if hasattr(X, \"toarray\"):\n",
    "    X = X.toarray()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1d35750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved to /dtu/blackhole/0e/214382/datasets/pbmc3k/pbmc3k_train_nb_autoencoder.pt\n",
      "Latent space saved to /dtu/blackhole/0e/214382/datasets/pbmc3k/pbmc3k_train_with_latent.h5ad\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Main training + encoding\n",
    "# -------------------------------\n",
    "input_file = input_file_path\n",
    "latent_dim = 50\n",
    "hidden_dims = [512, 256]\n",
    "batch_size = 512\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load data ---\n",
    "adata = ad.read_h5ad(input_file)\n",
    "\n",
    "# Load RAW COUNTS\n",
    "X = adata.layers[\"X_counts\"]\n",
    "if hasattr(X, \"toarray\"):\n",
    "    X = X.toarray()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# Dataset + DataLoader\n",
    "dataset = TensorDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- Initialize model ---\n",
    "num_genes = adata.n_vars\n",
    "model = NB_Autoencoder(num_features=num_genes,\n",
    "                       latent_dim=latent_dim,\n",
    "                       hidden_dims=hidden_dims)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Training loop ---\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_batch = batch[0].to(device)\n",
    "\n",
    "        outputs = model(x_batch)\n",
    "        loss_dict = model.loss_function(x_batch, outputs)\n",
    "        loss = loss_dict[\"loss\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    epoch_loss /= len(dataset)\n",
    "\n",
    "# Save model\n",
    "model_file = input_file_path.replace(\".h5ad\", \"_nb_autoencoder.pt\")\n",
    "torch.save(model.state_dict(), model_file)\n",
    "print(f\"Trained model saved to {model_file}\")\n",
    "\n",
    "# --- Encode cells ---\n",
    "model.eval()\n",
    "all_z = []\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(dataset, batch_size=batch_size):\n",
    "        x_batch = batch[0].to(device)\n",
    "        z = model(x_batch)[\"z\"].cpu().numpy()\n",
    "        all_z.append(z)\n",
    "\n",
    "latent = np.concatenate(all_z, axis=0)\n",
    "\n",
    "# Save to AnnData\n",
    "adata.obsm[\"X_latent\"] = latent\n",
    "output_file = input_file.replace(\".h5ad\", \"_with_latent.h5ad\")\n",
    "adata.write(output_file)\n",
    "\n",
    "print(f\"Latent space saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
