{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d0b75a",
   "metadata": {},
   "source": [
    "Isoflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7cbf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from typing import List, Optional, Callable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from autoencoder_utils import NB_Autoencoder\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, List, Type, Tuple, Dict\n",
    "import math\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.axes._axes import Axes\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "from torch.func import vmap, jacrev\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from pathlib import Path\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from flow_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741b2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "TRAIN_DATA_PATH = Path(\"/dtu/blackhole/06/213542/paperdata/pbmc3k_train.h5ad\")\n",
    "TEST_DATA_PATH = Path(\"/dtu/blackhole/06/213542/paperdata/pbmc3k_test.h5ad\")\n",
    "RESULTS_DATA_PATH = Path(\"/dtu/blackhole/06/213542/paperdata/\")\n",
    "RESULTS_DATA_PATH.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "adata = sc.read_h5ad(TRAIN_DATA_PATH)\n",
    "adata_test = sc.read_h5ad(TEST_DATA_PATH)\n",
    "\n",
    "#remove genes with low count\n",
    "#sc.pp.filter_genes(adata, min_cells=20)\n",
    "#sc.pp.filter_genes(adata_test, min_cells=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f6b2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 5837.239, NLL: 5400.990\n",
      "Epoch 2/5, Loss: 4941.993, NLL: 4514.593\n",
      "Epoch 3/5, Loss: 4193.137, NLL: 3856.201\n",
      "Epoch 4/5, Loss: 3700.656, NLL: 3548.700\n",
      "Epoch 5/5, Loss: 3452.399, NLL: 3385.501\n",
      "Trained model saved to /dtu/blackhole/06/213542/paperdata/pbmc3k_train_nb_autoencoder.pt\n",
      "Latent space saved to /dtu/blackhole/06/213542/paperdata/pbmc3k_train_with_latent.h5ad\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Main training + encoding\n",
    "# -------------------------------\n",
    "    # --- Hyperparameters ---\n",
    "input_file = adata\n",
    "latent_dim = 50\n",
    "hidden_dims = [512, 256]\n",
    "batch_size = 512\n",
    "epochs = 5            # short run to check\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = adata.X\n",
    "if hasattr(X, \"toarray\"):\n",
    "    X = X.toarray()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # --- Initialize model ---\n",
    "num_genes = adata.n_vars\n",
    "model = NB_Autoencoder(num_features=num_genes, latent_dim=latent_dim, hidden_dims=hidden_dims)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # --- Training loop ---\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_batch = batch[0].to(device)\n",
    "        outputs = model(x_batch)\n",
    "        loss_dict = model.loss_function(x_batch, outputs)\n",
    "        loss = loss_dict[\"loss\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    epoch_loss /= len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.3f}, NLL: {loss_dict['nll'].item():.3f}\")\n",
    "\n",
    "    # --- Save trained model ---\n",
    "model_file = TRAIN_DATA_PATH.with_name(TRAIN_DATA_PATH.stem + \"_nb_autoencoder.pt\")\n",
    "torch.save(model.state_dict(), model_file)\n",
    "print(f\"Trained model saved to {model_file}\")\n",
    "\n",
    "    # --- Encode all cells into latent space ---\n",
    "model.eval()\n",
    "all_z = []\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(dataset, batch_size=batch_size):\n",
    "        x_batch = batch[0].to(device)\n",
    "        z = model(x_batch)[\"z\"].cpu().numpy()\n",
    "        all_z.append(z)\n",
    "latent = np.concatenate(all_z, axis=0)\n",
    "\n",
    "    # --- Save latent space to AnnData ---\n",
    "adata.obsm[\"X_latent\"] = latent\n",
    "output_file = TRAIN_DATA_PATH.with_name(TRAIN_DATA_PATH.stem + \"_with_latent.h5ad\")\n",
    "adata.write(output_file)\n",
    "print(f\"Latent space saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eba9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load model ---\n",
    "num_genes = adata_test.n_vars\n",
    "model = NB_Autoencoder(num_features=num_genes, latent_dim=50, hidden_dims=[512, 256])\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Encode new cells\n",
    "X_new = adata_test.X\n",
    "if hasattr(X_new, \"toarray\"):\n",
    "    X_new = X_new.toarray()\n",
    "X_new = torch.tensor(X_new, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_new = model(X_new)[\"z\"].cpu().numpy()\n",
    "\n",
    "# Save to AnnData\n",
    "adata_test.obsm[\"X_latent\"] = z_new\n",
    "new_cells = RESULTS_DATA_PATH /\"new_cells_with_latent.h5ad\"\n",
    "adata_test.write(new_cells)\n",
    "\n",
    "print(f\"new cells are generated in  >>{new_cells}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1e0a8",
   "metadata": {},
   "source": [
    "Flow model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.read_h5ad(output_file)\n",
    "\n",
    "# Access latent representation\n",
    "latent = adata.obsm[\"X_latent\"]\n",
    "# make it to a tensor and save in GPU\n",
    "latent_tensor = torch.tensor(latent, dtype=torch.float32, device = device)\n",
    "print(\"Shape of latent space:\", latent.shape)\n",
    "print(latent[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e79fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dist = EmpiricalDistribution(latent_tensor)\n",
    "samples = dist.sample(3)\n",
    "logp = dist.log_density(samples)\n",
    "print(logp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to go with Gaussian probability path, therefore we need to load functions for alpha and beta\n",
    "alpha = LinearAlpha()\n",
    "beta = LinearBeta()\n",
    "path = GaussianConditionalProbabilityPath(\n",
    "    p_data=emp_dist,\n",
    "    alpha=alpha,\n",
    "    beta=beta\n",
    ")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccceecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we were able to construct a Gaussian probability path, we have to be able to make a conditional vector field\n",
    "cvf_ode = ConditionalVectorFieldODE(path, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size = 2110\n",
    "num_epochs = 5000\n",
    "learning_rate = 1e-3\n",
    "latent_dim = latent_tensor.shape[1]  # e.g., 50\n",
    "\n",
    "vf_model = NeuralVectorField(latent_dim=latent_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(vf_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize GaussianConditionalProbabilityPath and ConditionalVectorFieldODE\n",
    "path = GaussianConditionalProbabilityPath(emp_dist, alpha, beta)  # define alpha, beta\n",
    "cvf_ode = ConditionalVectorFieldODE(path, z=torch.zeros(1, latent_dim, device=device))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Sample conditioning variable z ---\n",
    "    z = emp_dist.sample(batch_size).to(device)\n",
    "\n",
    "    # --- Sample time ---\n",
    "    t = torch.rand(batch_size, 1, device=device)\n",
    "\n",
    "    # --- Sample x_t from conditional path ---\n",
    "    with torch.no_grad():\n",
    "        x = path.sample_conditional_path(z, t)\n",
    "        u_target = path.conditional_vector_field(x, z, t)\n",
    "\n",
    "    # --- Normalize target ---\n",
    "    u_mean = u_target.mean(dim=0, keepdim=True)\n",
    "    u_std = u_target.std(dim=0, keepdim=True) + 1e-6\n",
    "    u_target_norm = (u_target - u_mean) / u_std\n",
    "\n",
    "    # --- Forward pass ---\n",
    "    v_pred = vf_model(x, z, t)\n",
    "\n",
    "    # --- Loss ---\n",
    "    loss = F.mse_loss(v_pred, u_target_norm)\n",
    "\n",
    "    # --- Backprop ---\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(vf_model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"[{epoch}] Loss: {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55062f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the trained neural network\n",
    "learned_ode = LearnedVectorFieldODE(vf_model)\n",
    "\n",
    "# Save the wrapper\n",
    "torch.save(learned_ode, RESULTS_DATA_PATH / \"learned_ode.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples and latent dimension\n",
    "n_samples = 1000\n",
    "latent_dim = latent_tensor.shape[1]\n",
    "\n",
    "# Starting points (noise)\n",
    "x = torch.randn(n_samples, latent_dim, device=device)\n",
    "\n",
    "# Conditioning variable z\n",
    "# Single vector, broadcast to all samples\n",
    "z = torch.zeros(1, latent_dim, device=device)  # or z = emp_dist.sample(1)\n",
    "\n",
    "# Wrap the trained neural network as an ODE\n",
    "learned_ode = LearnedVectorFieldODE(vf_model)\n",
    "\n",
    "# Create Euler simulator with the conditioning variable\n",
    "simulator = EulerSimulator(learned_ode, z)\n",
    "\n",
    "# Simulation parameters\n",
    "t0, t1 = 0.0, 1.0\n",
    "n_steps = 50\n",
    "dt = (t1 - t0) / n_steps\n",
    "\n",
    "# Store trajectory\n",
    "trajectory = [x.clone()]\n",
    "t = torch.full((n_samples, 1), t0, device=device)\n",
    "\n",
    "# Euler integration\n",
    "for _ in range(n_steps):\n",
    "    x = simulator.step(x, t, dt)\n",
    "    trajectory.append(x.clone())\n",
    "    t = t + dt\n",
    "\n",
    "# Final generated samples\n",
    "generated_cells = trajectory[-1]\n",
    "print(generated_cells.shape)  # (1000, latent_dim)\n",
    "torch.save(generated_cells, RESULTS_DATA_PATH / \"generated_latent.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
