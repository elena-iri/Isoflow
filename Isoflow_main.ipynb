{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d0b75a",
   "metadata": {},
   "source": [
    "Isoflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7cbf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from typing import List, Optional, Callable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from autoencoder_utils import NB_Autoencoder\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, List, Type, Tuple, Dict\n",
    "import math\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.axes._axes import Axes\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "from torch.func import vmap, jacrev\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from pathlib import Path\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from flow_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741b2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "\n",
    "#this the the only part that needs to be edited before running\n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = Path(\"/dtu/blackhole/06/213542/paperdata/pbmc3k_train.h5ad\")\n",
    "TEST_DATA_PATH = Path(\"/dtu/blackhole/06/213542/paperdata/pbmc3k_test.h5ad\")\n",
    "RESULTS_DATA_PATH = Path(\"/dtu/blackhole/06/213542/paperdata/\")\n",
    "RESULTS_DATA_PATH.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "adata = sc.read_h5ad(TRAIN_DATA_PATH)\n",
    "adata_test = sc.read_h5ad(TEST_DATA_PATH)\n",
    "\n",
    "#remove genes with low count\n",
    "#sc.pp.filter_genes(adata, min_cells=20)\n",
    "#sc.pp.filter_genes(adata_test, min_cells=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f6b2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 5825.952, NLL: 5361.806\n",
      "Epoch 2/5, Loss: 4924.762, NLL: 4523.493\n",
      "Epoch 3/5, Loss: 4185.058, NLL: 3946.390\n",
      "Epoch 4/5, Loss: 3705.969, NLL: 3730.872\n",
      "Epoch 5/5, Loss: 3468.197, NLL: 3503.920\n",
      "Trained model saved to /dtu/blackhole/06/213542/paperdata/pbmc3k_train_nb_autoencoder.pt\n",
      "Latent space saved to /dtu/blackhole/06/213542/paperdata/pbmc3k_train_with_latent.h5ad\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Main training + encoding\n",
    "# -------------------------------\n",
    "    # --- Hyperparameters ---\n",
    "input_file = adata\n",
    "latent_dim = 50\n",
    "hidden_dims = [512, 256]\n",
    "batch_size = 512\n",
    "epochs = 5            # short run to check\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = adata.X\n",
    "if hasattr(X, \"toarray\"):\n",
    "    X = X.toarray()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # --- Initialize model ---\n",
    "num_genes = adata.n_vars\n",
    "model = NB_Autoencoder(num_features=num_genes, latent_dim=latent_dim, hidden_dims=hidden_dims)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # --- Training loop ---\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_batch = batch[0].to(device)\n",
    "        outputs = model(x_batch)\n",
    "        loss_dict = model.loss_function(x_batch, outputs)\n",
    "        loss = loss_dict[\"loss\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    epoch_loss /= len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.3f}, NLL: {loss_dict['nll'].item():.3f}\")\n",
    "\n",
    "    # --- Save trained model ---\n",
    "model_file = TRAIN_DATA_PATH.with_name(TRAIN_DATA_PATH.stem + \"_nb_autoencoder.pt\")\n",
    "torch.save(model.state_dict(), model_file)\n",
    "print(f\"Trained model saved to {model_file}\")\n",
    "\n",
    "    # --- Encode all cells into latent space ---\n",
    "model.eval()\n",
    "all_z = []\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(dataset, batch_size=batch_size):\n",
    "        x_batch = batch[0].to(device)\n",
    "        z = model(x_batch)[\"z\"].cpu().numpy()\n",
    "        all_z.append(z)\n",
    "latent = np.concatenate(all_z, axis=0)\n",
    "\n",
    "    # --- Save latent space to AnnData ---\n",
    "adata.obsm[\"X_latent\"] = latent\n",
    "output_file = TRAIN_DATA_PATH.with_name(TRAIN_DATA_PATH.stem + \"_with_latent.h5ad\")\n",
    "adata.write(output_file)\n",
    "print(f\"Latent space saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58eba9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new cells are generated in  >>/dtu/blackhole/06/213542/paperdata/new_cells_with_latent.h5ad\n"
     ]
    }
   ],
   "source": [
    "# --- Load model ---\n",
    "num_genes = adata_test.n_vars\n",
    "model = NB_Autoencoder(num_features=num_genes, latent_dim=50, hidden_dims=[512, 256])\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Encode new cells\n",
    "X_new = adata_test.X\n",
    "if hasattr(X_new, \"toarray\"):\n",
    "    X_new = X_new.toarray()\n",
    "X_new = torch.tensor(X_new, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_new = model(X_new)[\"z\"].cpu().numpy()\n",
    "\n",
    "# Save to AnnData\n",
    "adata_test.obsm[\"X_latent\"] = z_new\n",
    "new_cells = RESULTS_DATA_PATH /\"new_cells_with_latent.h5ad\"\n",
    "adata_test.write(new_cells)\n",
    "\n",
    "print(f\"new cells are generated in  >>{new_cells}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1e0a8",
   "metadata": {},
   "source": [
    "Flow model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d592903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of latent space: (2110, 50)\n",
      "[[ 7.2119427e-01  4.5030427e+00  4.2068303e-02 -1.2502990e+00\n",
      "  -4.2017937e+00 -2.4717948e+00 -1.5488725e+00 -1.8064263e+00\n",
      "   8.0419064e-02  1.3857546e+00  1.5554056e+00 -3.6009327e-01\n",
      "  -3.2110305e+00 -6.2240481e-01 -1.2413149e+00 -1.1609215e+00\n",
      "   3.6916310e-01  1.0324364e+00  4.2429629e-01 -1.3808545e+00\n",
      "  -2.6205373e+00  1.0236942e+00  2.3036060e+00  2.9912877e+00\n",
      "   1.8578960e+00 -1.3467599e+00 -2.4267607e+00  4.2268958e+00\n",
      "   6.7074037e-01 -3.4374635e+00  1.6940813e+00  1.2496483e+00\n",
      "  -1.0571158e+00  2.6391823e+00 -3.2057946e+00  6.9281155e-01\n",
      "  -1.6405296e+00 -4.2034798e+00  1.0298922e+00  3.6848754e-01\n",
      "   1.9148359e+00  5.8723283e-01 -8.6443865e-01 -4.0662823e+00\n",
      "  -1.6493044e+00  8.4716082e-04 -4.6969833e+00 -4.1647023e-01\n",
      "  -1.7651422e+00  3.6352050e-01]\n",
      " [ 1.0745261e+00  4.1269851e+00  6.6678226e-02 -1.6734134e+00\n",
      "  -5.4810619e+00 -2.8954308e+00 -1.4715413e+00 -1.3079782e+00\n",
      "  -2.7325594e-01  1.6386645e+00  1.7486247e+00  8.0161452e-01\n",
      "  -4.4936929e+00 -3.8353905e-02 -1.6085614e+00 -5.5215657e-01\n",
      "  -1.9759455e-01  1.0296991e+00 -2.0799027e-01 -1.7178960e+00\n",
      "  -2.7860963e+00  1.5485073e+00  2.6643701e+00  3.5747976e+00\n",
      "   1.9924279e+00 -1.3459471e+00 -2.4070220e+00  5.6537247e+00\n",
      "   4.2843932e-01 -3.2062492e+00  1.6869358e+00  1.3366416e+00\n",
      "  -1.2508645e+00  3.3051152e+00 -3.2002869e+00  7.2467721e-01\n",
      "  -1.6174685e+00 -3.5157034e+00  1.0170336e+00 -2.6944903e-01\n",
      "   2.0492508e+00  3.7766004e-01 -1.3154323e+00 -4.4302468e+00\n",
      "  -1.2350175e+00 -4.5528743e-01 -4.7889853e+00 -6.7045915e-01\n",
      "  -2.3756528e+00  2.1261144e-01]\n",
      " [-1.0466746e+00 -1.8226492e+00  4.0546802e-01  1.6915082e+00\n",
      "   2.3696980e+00  6.6516823e-01 -6.9607633e-01 -6.9606811e-01\n",
      "  -1.7667976e-01 -4.8587242e-01  4.3639854e-01  5.1583213e-01\n",
      "   1.5750997e+00  1.3472445e+00 -1.3006918e+00 -1.6271253e+00\n",
      "   2.0638704e+00 -1.0100759e+00 -4.9079213e-01  7.1968997e-01\n",
      "   1.2276487e+00  5.6502783e-01 -1.2530133e+00 -1.8185736e+00\n",
      "  -1.2216690e+00 -6.0791659e-01  1.7620733e+00 -2.2457747e+00\n",
      "  -1.3464886e+00  6.4577001e-01 -1.4780318e+00  5.4318225e-01\n",
      "   2.8729630e-01 -1.2624631e+00 -8.1681877e-01 -8.9502442e-01\n",
      "   5.2279913e-01  5.9781802e-01  3.1886178e-01  2.0749907e+00\n",
      "   9.4657958e-01 -6.7802781e-01  1.9582438e-01 -1.3046353e+00\n",
      "   1.4956378e+00  2.7166557e-01  2.1274123e+00  1.8821318e-01\n",
      "  -4.6411049e-01  1.1962277e+00]\n",
      " [ 4.8234558e-01  1.4052641e+00 -2.5865158e-02  1.0900090e+00\n",
      "   5.9698439e-01  7.0452750e-01 -3.1664228e-01  9.7569382e-01\n",
      "   1.6905387e-01 -6.5537727e-01 -9.3412012e-01 -3.9990237e+00\n",
      "   3.7509131e+00 -1.5949578e+00  1.6448293e+00  2.7503672e+00\n",
      "  -2.1698580e+00  5.3737104e-02  1.3972559e+00  1.1150090e+00\n",
      "   1.4893413e-03 -1.4964354e+00  5.7390857e-01 -2.1925437e+00\n",
      "   4.0021226e-01 -2.1118411e-01 -3.0141219e-01 -3.0038223e+00\n",
      "   8.5491025e-01 -1.2161642e-02  9.4422114e-01 -2.5262883e+00\n",
      "   7.2965473e-02 -1.2013143e+00  2.0174124e+00  1.5413213e+00\n",
      "   9.5241570e-01  1.2643905e+00 -6.3330096e-01 -1.7025176e+00\n",
      "  -2.2245889e+00  1.3285751e+00 -7.0985121e-01  5.4283905e+00\n",
      "  -1.2277792e+00  2.7346723e+00  2.1763294e+00  1.5673627e+00\n",
      "   2.0283122e+00 -1.5523858e+00]\n",
      " [ 6.8674511e-01  4.1236830e+00  1.1088371e-02 -1.2821331e+00\n",
      "  -4.2757411e+00 -2.4240258e+00 -1.5441375e+00 -1.4391925e+00\n",
      "  -1.0367119e-01  1.5655606e+00  1.3660324e+00 -1.4249152e-01\n",
      "  -3.3110998e+00 -3.1852534e-01 -1.5388315e+00 -9.8559034e-01\n",
      "   2.9804462e-01  6.7731714e-01  3.1295556e-01 -1.5107975e+00\n",
      "  -2.4936643e+00  1.2376070e+00  2.1246245e+00  2.8347971e+00\n",
      "   1.5590470e+00 -1.3292335e+00 -2.0266099e+00  4.0291152e+00\n",
      "   3.3718333e-01 -3.1476707e+00  1.7033774e+00  1.1804545e+00\n",
      "  -9.3213385e-01  2.6332397e+00 -3.0638809e+00  7.8605121e-01\n",
      "  -1.3912537e+00 -3.4943116e+00  8.3853257e-01  6.2943667e-02\n",
      "   1.7713871e+00  5.7248497e-01 -1.1467466e+00 -3.7045131e+00\n",
      "  -1.4652345e+00  2.4297401e-01 -4.0025358e+00 -3.6385846e-01\n",
      "  -1.9185944e+00  4.3294865e-01]]\n"
     ]
    }
   ],
   "source": [
    "adata = ad.read_h5ad(output_file)\n",
    "\n",
    "# Access latent representation\n",
    "latent = adata.obsm[\"X_latent\"]\n",
    "# make it to a tensor and save in GPU\n",
    "latent_tensor = torch.tensor(latent, dtype=torch.float32, device = device)\n",
    "print(\"Shape of latent space:\", latent.shape)\n",
    "print(latent[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e79fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-58.9096],\n",
      "        [-59.2401],\n",
      "        [-58.4858]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dist = EmpiricalDistribution(latent_tensor)\n",
    "emp_dist = dist\n",
    "samples = dist.sample(3)\n",
    "logp = dist.log_density(samples)\n",
    "print(logp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9be5db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<flow_utils.GaussianConditionalProbabilityPath object at 0x7fee2b9d3910>\n"
     ]
    }
   ],
   "source": [
    "# We want to go with Gaussian probability path, therefore we need to load functions for alpha and beta\n",
    "alpha = LinearAlpha()\n",
    "beta = LinearBeta()\n",
    "path = GaussianConditionalProbabilityPath(\n",
    "    p_data=emp_dist,\n",
    "    #==============\n",
    "    #emp_dist? - i was not sure what that should be so i set it as same as dist in a box above\n",
    "    #==============\n",
    "    alpha=alpha,\n",
    "    beta=beta\n",
    ")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ccceecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we were able to construct a Gaussian probability path, we have to be able to make a conditional vector field\n",
    "cvf_ode = ConditionalVectorFieldODE(path, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e30f51e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Loss: 1.171783\n",
      "[50] Loss: 0.181398\n",
      "[100] Loss: 0.125297\n",
      "[150] Loss: 0.085925\n",
      "[200] Loss: 0.070035\n",
      "[250] Loss: 0.059561\n",
      "[300] Loss: 0.051298\n",
      "[350] Loss: 0.045498\n",
      "[400] Loss: 0.045723\n",
      "[450] Loss: 0.038390\n",
      "[500] Loss: 0.034966\n",
      "[550] Loss: 0.033451\n",
      "[600] Loss: 0.033136\n",
      "[650] Loss: 0.034926\n",
      "[700] Loss: 0.030230\n",
      "[750] Loss: 0.028998\n",
      "[800] Loss: 0.024815\n",
      "[850] Loss: 0.029459\n",
      "[900] Loss: 0.027135\n",
      "[950] Loss: 0.027422\n",
      "[1000] Loss: 0.022736\n",
      "[1050] Loss: 0.024817\n",
      "[1100] Loss: 0.019597\n",
      "[1150] Loss: 0.025040\n",
      "[1200] Loss: 0.023449\n",
      "[1250] Loss: 0.022704\n",
      "[1300] Loss: 0.024265\n",
      "[1350] Loss: 0.019104\n",
      "[1400] Loss: 0.019681\n",
      "[1450] Loss: 0.019978\n",
      "[1500] Loss: 0.018364\n",
      "[1550] Loss: 0.020742\n",
      "[1600] Loss: 0.019584\n",
      "[1650] Loss: 0.021289\n",
      "[1700] Loss: 0.022086\n",
      "[1750] Loss: 0.017069\n",
      "[1800] Loss: 0.017599\n",
      "[1850] Loss: 0.018518\n",
      "[1900] Loss: 0.020248\n",
      "[1950] Loss: 0.019541\n",
      "[2000] Loss: 0.018724\n",
      "[2050] Loss: 0.019119\n",
      "[2100] Loss: 0.015829\n",
      "[2150] Loss: 0.021062\n",
      "[2200] Loss: 0.018477\n",
      "[2250] Loss: 0.019520\n",
      "[2300] Loss: 0.016390\n",
      "[2350] Loss: 0.017518\n",
      "[2400] Loss: 0.015916\n",
      "[2450] Loss: 0.016150\n",
      "[2500] Loss: 0.016773\n",
      "[2550] Loss: 0.014282\n",
      "[2600] Loss: 0.014743\n",
      "[2650] Loss: 0.013804\n",
      "[2700] Loss: 0.018177\n",
      "[2750] Loss: 0.018832\n",
      "[2800] Loss: 0.015064\n",
      "[2850] Loss: 0.015943\n",
      "[2900] Loss: 0.015442\n",
      "[2950] Loss: 0.018990\n",
      "[3000] Loss: 0.014323\n",
      "[3050] Loss: 0.016192\n",
      "[3100] Loss: 0.014331\n",
      "[3150] Loss: 0.015751\n",
      "[3200] Loss: 0.016728\n",
      "[3250] Loss: 0.014260\n",
      "[3300] Loss: 0.018254\n",
      "[3350] Loss: 0.015590\n",
      "[3400] Loss: 0.015872\n",
      "[3450] Loss: 0.015899\n",
      "[3500] Loss: 0.013157\n",
      "[3550] Loss: 0.014403\n",
      "[3600] Loss: 0.013736\n",
      "[3650] Loss: 0.015535\n",
      "[3700] Loss: 0.016480\n",
      "[3750] Loss: 0.014317\n",
      "[3800] Loss: 0.014588\n",
      "[3850] Loss: 0.014308\n",
      "[3900] Loss: 0.016566\n",
      "[3950] Loss: 0.016237\n",
      "[4000] Loss: 0.018618\n",
      "[4050] Loss: 0.012704\n",
      "[4100] Loss: 0.012995\n",
      "[4150] Loss: 0.013563\n",
      "[4200] Loss: 0.014864\n",
      "[4250] Loss: 0.013754\n",
      "[4300] Loss: 0.013173\n",
      "[4350] Loss: 0.015185\n",
      "[4400] Loss: 0.014019\n",
      "[4450] Loss: 0.013118\n",
      "[4500] Loss: 0.013763\n",
      "[4550] Loss: 0.013787\n",
      "[4600] Loss: 0.016461\n",
      "[4650] Loss: 0.011825\n",
      "[4700] Loss: 0.014137\n",
      "[4750] Loss: 0.012984\n",
      "[4800] Loss: 0.014364\n",
      "[4850] Loss: 0.014202\n",
      "[4900] Loss: 0.013448\n",
      "[4950] Loss: 0.014051\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size = 2110\n",
    "num_epochs = 5000\n",
    "learning_rate = 1e-3\n",
    "latent_dim = latent_tensor.shape[1]  # e.g., 50\n",
    "\n",
    "vf_model = NeuralVectorField(latent_dim=latent_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(vf_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize GaussianConditionalProbabilityPath and ConditionalVectorFieldODE\n",
    "path = GaussianConditionalProbabilityPath(emp_dist, alpha, beta)  # define alpha, beta\n",
    "cvf_ode = ConditionalVectorFieldODE(path, z=torch.zeros(1, latent_dim, device=device))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Sample conditioning variable z ---\n",
    "    z = emp_dist.sample(batch_size).to(device)\n",
    "\n",
    "    # --- Sample time ---\n",
    "    t = torch.rand(batch_size, 1, device=device)\n",
    "\n",
    "    # --- Sample x_t from conditional path ---\n",
    "    with torch.no_grad():\n",
    "        x = path.sample_conditional_path(z, t)\n",
    "        u_target = path.conditional_vector_field(x, z, t)\n",
    "\n",
    "    # --- Normalize target ---\n",
    "    u_mean = u_target.mean(dim=0, keepdim=True)\n",
    "    u_std = u_target.std(dim=0, keepdim=True) + 1e-6\n",
    "    u_target_norm = (u_target - u_mean) / u_std\n",
    "\n",
    "    # --- Forward pass ---\n",
    "    v_pred = vf_model(x, z, t)\n",
    "\n",
    "    # --- Loss ---\n",
    "    loss = F.mse_loss(v_pred, u_target_norm)\n",
    "\n",
    "    # --- Backprop ---\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(vf_model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"[{epoch}] Loss: {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55062f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the trained neural network\n",
    "learned_ode = LearnedVectorFieldODE(vf_model)\n",
    "\n",
    "# Save the wrapper\n",
    "torch.save(learned_ode, RESULTS_DATA_PATH / \"learned_ode.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e84c727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 50])\n"
     ]
    }
   ],
   "source": [
    "# Number of samples and latent dimension\n",
    "n_samples = 1000\n",
    "latent_dim = latent_tensor.shape[1]\n",
    "\n",
    "# Starting points (noise)\n",
    "x = torch.randn(n_samples, latent_dim, device=device)\n",
    "\n",
    "# Conditioning variable z\n",
    "# Single vector, broadcast to all samples\n",
    "z = torch.zeros(1, latent_dim, device=device)  # or z = emp_dist.sample(1)\n",
    "\n",
    "# Wrap the trained neural network as an ODE\n",
    "learned_ode = LearnedVectorFieldODE(vf_model)\n",
    "\n",
    "# Create Euler simulator with the conditioning variable\n",
    "simulator = EulerSimulator(learned_ode, z)\n",
    "\n",
    "# Simulation parameters\n",
    "t0, t1 = 0.0, 1.0\n",
    "n_steps = 50\n",
    "dt = (t1 - t0) / n_steps\n",
    "\n",
    "# Store trajectory\n",
    "trajectory = [x.clone()]\n",
    "t = torch.full((n_samples, 1), t0, device=device)\n",
    "\n",
    "# Euler integration\n",
    "for _ in range(n_steps):\n",
    "    x = simulator.step(x, t, dt)\n",
    "    trajectory.append(x.clone())\n",
    "    t = t + dt\n",
    "\n",
    "# Final generated samples\n",
    "generated_cells = trajectory[-1]\n",
    "print(generated_cells.shape)  # (1000, latent_dim)\n",
    "torch.save(generated_cells, RESULTS_DATA_PATH / \"generated_latent.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e4210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4533e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccdc03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
