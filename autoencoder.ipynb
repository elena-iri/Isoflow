{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22e427f",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f6efc0",
   "metadata": {},
   "source": [
    "We inspired our encoder in the one used in CFGEN, which has is a multi-layer perceptron (MLP) with two hidden layers of dimensions\n",
    "[512, 256] that map the input to a 50-dimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622c038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from typing import List, Optional, Callable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c7d4a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Define MLP (like the one in CFGEN)\n",
    "# -------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dims: List[int],\n",
    "                 batch_norm: bool = True, \n",
    "                 dropout: bool = True, \n",
    "                 dropout_p: float = 0.1, \n",
    "                 activation: Optional[Callable] = nn.ELU, \n",
    "                 final_activation: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        layers = []\n",
    "        for i in range(len(dims[:-2])):\n",
    "            block = [nn.Linear(dims[i], dims[i+1])]\n",
    "            if batch_norm:\n",
    "                block.append(nn.BatchNorm1d(dims[i+1]))\n",
    "            block.append(activation())\n",
    "            if dropout:\n",
    "                block.append(nn.Dropout(dropout_p))\n",
    "            layers.append(nn.Sequential(*block))\n",
    "        layers.append(nn.Linear(dims[-2], dims[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        if final_activation == \"tanh\":\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_activation == \"sigmoid\":\n",
    "            self.final_activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.final_activation = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x if self.final_activation is None else self.final_activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb3bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Negative Binomial log-likelihood\n",
    "# -------------------------------\n",
    "def negative_binomial_log_likelihood(x, mu, theta, eps=1e-8):\n",
    "    t1 = torch.lgamma(theta + eps) + torch.lgamma(x + 1.0) - torch.lgamma(x + theta + eps)\n",
    "    t2 = (theta * (torch.log(theta + eps) - torch.log(mu + theta + eps))) + \\\n",
    "         (x * (torch.log(mu + eps) - torch.log(mu + theta + eps)))\n",
    "    return t1 + t2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5233f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# NB Autoencoder\n",
    "# -------------------------------\n",
    "class NB_Autoencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 latent_dim: int = 50,\n",
    "                 hidden_dims: List[int] = [512, 256],\n",
    "                 dropout_p: float = 0.1,\n",
    "                 l2_reg: float = 1e-5,\n",
    "                 kl_reg: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.l2_reg = l2_reg\n",
    "        self.kl_reg = kl_reg\n",
    "\n",
    "        self.encoder = MLP(\n",
    "            dims=[num_features, *hidden_dims, latent_dim],\n",
    "            batch_norm=True,\n",
    "            dropout=True,\n",
    "            dropout_p=dropout_p\n",
    "        )\n",
    "\n",
    "        self.decoder = MLP(\n",
    "            dims=[latent_dim, *hidden_dims[::-1], num_features],\n",
    "            batch_norm=True,\n",
    "            dropout=True,\n",
    "            dropout_p=dropout_p\n",
    "        )\n",
    "\n",
    "        self.log_theta = nn.Parameter(torch.randn(num_features) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        mu = F.softplus(self.decoder(z))\n",
    "        theta = torch.exp(self.log_theta).unsqueeze(0).expand_as(mu)\n",
    "        return {\"z\": z, \"mu\": mu, \"theta\": theta}\n",
    "\n",
    "    def loss_function(self, x, outputs):\n",
    "        mu = outputs[\"mu\"]\n",
    "        theta = outputs[\"theta\"]\n",
    "        z = outputs[\"z\"]\n",
    "        nll = -negative_binomial_log_likelihood(x, mu, theta).sum(dim=1).mean()\n",
    "        l2_loss = sum((p**2).sum() for p in self.parameters()) * self.l2_reg\n",
    "        kl_loss = (z**2).mean() * self.kl_reg\n",
    "        loss = nll + l2_loss + kl_loss\n",
    "        return {\"loss\": loss, \"nll\": nll, \"l2\": l2_loss, \"kl\": kl_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4d9b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_genes</th>\n",
       "      <th>percent_mito</th>\n",
       "      <th>n_counts</th>\n",
       "      <th>cell_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CCAATTTGAACGTC-1</th>\n",
       "      <td>932</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>2671.0</td>\n",
       "      <td>CD14+ Monocytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AACAGCACAAGAGT-1</th>\n",
       "      <td>343</td>\n",
       "      <td>0.024709</td>\n",
       "      <td>688.0</td>\n",
       "      <td>CD14+ Monocytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGAGATGACTGAAC-1</th>\n",
       "      <td>678</td>\n",
       "      <td>0.023834</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>CD4 T cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AATAGGGAGAATGA-1</th>\n",
       "      <td>756</td>\n",
       "      <td>0.036002</td>\n",
       "      <td>1861.0</td>\n",
       "      <td>CD8 T cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCCTGACTCTCAAG-1</th>\n",
       "      <td>757</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>CD14+ Monocytes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  n_genes  percent_mito  n_counts        cell_type\n",
       "index                                                             \n",
       "CCAATTTGAACGTC-1      932      0.010483    2671.0  CD14+ Monocytes\n",
       "AACAGCACAAGAGT-1      343      0.024709     688.0  CD14+ Monocytes\n",
       "AGAGATGACTGAAC-1      678      0.023834    1972.0      CD4 T cells\n",
       "AATAGGGAGAATGA-1      756      0.036002    1861.0      CD8 T cells\n",
       "GCCTGACTCTCAAG-1      757      0.014493    2001.0  CD14+ Monocytes"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if it's corrupted\n",
    "import anndata as ad\n",
    "input_file_path = \"/dtu/blackhole/06/213542/paperdata/pbmc3k_train.h5ad\"\n",
    "#adata = ad.read_h5ad(\"/zhome/99/9/213566/deepL/adata_preprocessed_subset.h5ad\", backed='r')\n",
    "adata = ad.read_h5ad(input_file_path, backed='r')\n",
    "adata.obs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d35750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 5828.489, NLL: 5350.439\n",
      "Epoch 2/5, Loss: 4916.502, NLL: 4521.828\n",
      "Epoch 3/5, Loss: 4164.151, NLL: 3874.815\n",
      "Epoch 4/5, Loss: 3688.528, NLL: 3644.679\n",
      "Epoch 5/5, Loss: 3449.235, NLL: 3441.099\n",
      "Trained model saved to /dtu/blackhole/06/213542/paperdata/pbmc3k_train_nb_autoencoder.pt\n",
      "Latent space saved to /dtu/blackhole/06/213542/paperdata/pbmc3k_train_with_latent.h5ad\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Main training + encoding\n",
    "# -------------------------------\n",
    "    # --- Hyperparameters ---\n",
    "input_file = input_file_path\n",
    "latent_dim = 50\n",
    "hidden_dims = [512, 256]\n",
    "batch_size = 512\n",
    "epochs = 5            # short run to check\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- Load data ---\n",
    "adata = anndata.read_h5ad(input_file)\n",
    "X = adata.X\n",
    "if hasattr(X, \"toarray\"):\n",
    "    X = X.toarray()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # --- Initialize model ---\n",
    "num_genes = adata.n_vars\n",
    "model = NB_Autoencoder(num_features=num_genes, latent_dim=latent_dim, hidden_dims=hidden_dims)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # --- Training loop ---\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_batch = batch[0].to(device)\n",
    "        outputs = model(x_batch)\n",
    "        loss_dict = model.loss_function(x_batch, outputs)\n",
    "        loss = loss_dict[\"loss\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    epoch_loss /= len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.3f}, NLL: {loss_dict['nll'].item():.3f}\")\n",
    "\n",
    "    # --- Save trained model ---\n",
    "model_file = input_file_path.replace(\".h5ad\", \"_nb_autoencoder.pt\")\n",
    "torch.save(model.state_dict(), model_file)\n",
    "print(f\"Trained model saved to {model_file}\")\n",
    "\n",
    "    # --- Encode all cells into latent space ---\n",
    "model.eval()\n",
    "all_z = []\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(dataset, batch_size=batch_size):\n",
    "        x_batch = batch[0].to(device)\n",
    "        z = model(x_batch)[\"z\"].cpu().numpy()\n",
    "        all_z.append(z)\n",
    "latent = np.concatenate(all_z, axis=0)\n",
    "\n",
    "    # --- Save latent space to AnnData ---\n",
    "adata.obsm[\"X_latent\"] = latent\n",
    "output_file = input_file.replace(\".h5ad\", \"_with_latent.h5ad\")\n",
    "adata.write(output_file)\n",
    "print(f\"Latent space saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6e980",
   "metadata": {},
   "source": [
    "Running on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f7366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path2 = \"/dtu/blackhole/06/213542/paperdata/pbmc3k_test.h5ad\"\n",
    "new_adata = anndata.read_h5ad(input_file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0466e0a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Unable to synchronously create file (unable to open file: name = '/dtu/blackhole/1e/213566/datasets/pbmc3k/new_cells_with_latent.h5ad', errno = 13, error message = 'Permission denied', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Save to AnnData\u001b[39;00m\n\u001b[32m     18\u001b[39m new_adata.obsm[\u001b[33m\"\u001b[39m\u001b[33mX_latent\u001b[39m\u001b[33m\"\u001b[39m] = z_new\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mnew_adata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/dtu/blackhole/1e/213566/datasets/pbmc3k/new_cells_with_latent.h5ad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_1/lib/python3.11/site-packages/anndata/_core/anndata.py:1903\u001b[39m, in \u001b[36mAnnData.write_h5ad\u001b[39m\u001b[34m(self, filename, convert_strings_to_categoricals, compression, compression_opts, as_dense)\u001b[39m\n\u001b[32m   1900\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1901\u001b[39m     filename = \u001b[38;5;28mself\u001b[39m.filename\n\u001b[32m-> \u001b[39m\u001b[32m1903\u001b[39m \u001b[43mwrite_h5ad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_strings_to_categoricals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_strings_to_categoricals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression_opts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_dense\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_dense\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[38;5;66;03m# Only reset the filename if the AnnData object now points to a complete new copy\u001b[39;00m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.isbacked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_view:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_1/lib/python3.11/site-packages/anndata/_io/utils.py:320\u001b[39m, in \u001b[36mno_write_dataset_2d.<locals>.raise_error_if_dataset_2d_present\u001b[39m\u001b[34m(store, adata, *args, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m     msg = (\n\u001b[32m    314\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWriting AnnData objects with a Dataset2D not supported yet. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    315\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease use `ds.to_memory` to bring the dataset into memory. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    316\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNote that if you have generated this object by concatenating several `AnnData` objects\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    317\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe original types may be lost.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    318\u001b[39m     )\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_1/lib/python3.11/site-packages/anndata/_io/h5ad.py:81\u001b[39m, in \u001b[36mwrite_h5ad\u001b[39m\u001b[34m(filepath, adata, as_dense, convert_strings_to_categoricals, dataset_kwargs, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adata.isbacked:  \u001b[38;5;66;03m# close so that we can reopen below\u001b[39;00m\n\u001b[32m     79\u001b[39m     adata.file.close()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# TODO: Use spec writing system for this\u001b[39;00m\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# Currently can't use write_dispatched here because this function is also called to do an\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# inplace update of a backed object, which would delete \"/\"\u001b[39;00m\n\u001b[32m     85\u001b[39m     f = cast(\u001b[33m\"\u001b[39m\u001b[33mh5py.Group\u001b[39m\u001b[33m\"\u001b[39m, f[\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     86\u001b[39m     f.attrs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mencoding-type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33manndata\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_1/lib/python3.11/site-packages/h5py/_hl/files.py:566\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, track_times, **kwds)\u001b[39m\n\u001b[32m    557\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[32m    558\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[32m    559\u001b[39m                      alignment_threshold=alignment_threshold,\n\u001b[32m    560\u001b[39m                      alignment_interval=alignment_interval,\n\u001b[32m    561\u001b[39m                      meta_block_size=meta_block_size,\n\u001b[32m    562\u001b[39m                      **kwds)\n\u001b[32m    563\u001b[39m     fcpl = make_fcpl(track_order=track_order, track_times=track_times,\n\u001b[32m    564\u001b[39m                      fs_strategy=fs_strategy, fs_persist=fs_persist,\n\u001b[32m    565\u001b[39m                      fs_threshold=fs_threshold, fs_page_size=fs_page_size)\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv_1/lib/python3.11/site-packages/h5py/_hl/files.py:247\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n\u001b[32m    245\u001b[39m     fid = h5f.create(name, h5f.ACC_EXCL, fapl=fapl, fcpl=fcpl)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mACC_TRUNC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    249\u001b[39m     \u001b[38;5;66;03m# Open in append mode (read/write).\u001b[39;00m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;66;03m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:124\u001b[39m, in \u001b[36mh5py.h5f.create\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Unable to synchronously create file (unable to open file: name = '/dtu/blackhole/1e/213566/datasets/pbmc3k/new_cells_with_latent.h5ad', errno = 13, error message = 'Permission denied', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "# --- Load model ---\n",
    "num_genes = new_adata.n_vars\n",
    "model = NB_Autoencoder(num_features=num_genes, latent_dim=50, hidden_dims=[512, 256])\n",
    "model.load_state_dict(torch.load(\"/dtu/blackhole/06/213542/paperdata/pbmc3k_train_nb_autoencoder.pt\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Encode new cells\n",
    "X_new = new_adata.X\n",
    "if hasattr(X_new, \"toarray\"):\n",
    "    X_new = X_new.toarray()\n",
    "X_new = torch.tensor(X_new, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_new = model(X_new)[\"z\"].cpu().numpy()\n",
    "\n",
    "# Save to AnnData\n",
    "new_adata.obsm[\"X_latent\"] = z_new\n",
    "new_adata.write(\"/dtu/blackhole/1e/213566/datasets/pbmc3k/new_cells_with_latent.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5c248-36e3-4119-b294-53b52d2cdde0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d3c66-4197-4a4b-a174-e5674eb8fd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
