{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f451dddd",
   "metadata": {},
   "source": [
    "# Flow Matching with Classifier-Free Guidance & Library Size Conditioning\n",
    "\n",
    "**Updates:**\n",
    "1. **Training:** Uses OT-Flow Matching (Gaussian Conditional Probability Path).\n",
    "2. **Conditioning:** Conditions on both **Cell Type** and **Library Size**.\n",
    "3. **Sampling:** Uses Euler ODE integration (Victor's method).\n",
    "4. **Decoding:** Rescales latents and decodes to counts using Negative Binomial (Victor's method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721fb7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, List, Tuple\n",
    "from scvi.distributions import NegativeBinomial\n",
    "\n",
    "# Imports for VAE (Ensure autoencoder_utils.py is in the same directory)\n",
    "try:\n",
    "    from autoencoder_utils import NB_Autoencoder\n",
    "except ImportError:\n",
    "    # Fallback if file not found, defining minimal class for loading\n",
    "    class NB_Autoencoder(nn.Module):\n",
    "        def __init__(self, num_features, latent_dim=50, hidden_dims=[512, 256]):\n",
    "            super().__init__()\n",
    "            self.num_features = num_features\n",
    "            self.latent_dim = latent_dim\n",
    "            # ... (Architecture assumed match)\n",
    "            # Placeholder for loading state dict\n",
    "            self.encoder = nn.Sequential(nn.Linear(num_features, 512), nn.ReLU(), nn.Linear(512, 256), nn.ReLU())\n",
    "            self.mu = nn.Linear(256, latent_dim)\n",
    "            self.logvar = nn.Linear(256, latent_dim)\n",
    "            self.decoder = nn.Sequential(nn.Linear(latent_dim, 256), nn.ReLU(), nn.Linear(256, 512), nn.ReLU())\n",
    "            self.mu_dec = nn.Linear(512, num_features)\n",
    "            self.theta = nn.Parameter(torch.randn(num_features))\n",
    "        def decode(self, z, library_size):\n",
    "            h = self.decoder(z)\n",
    "            mu = torch.softmax(self.mu_dec(h), dim=1) * library_size\n",
    "            return {\"mu\": mu, \"theta\": self.theta}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79499e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Configuration & Paths ----\n",
    "input_file_path = \"/dtu/blackhole/06/213542/paperdata/pbmc3k_train_with_latent.h5ad\"\n",
    "vae_model_path = \"/dtu/blackhole/06/213542/paperdata/pbmc3k_train_nb_autoencoder.pt\" # Path to trained VAE\n",
    "flow_model_save_path = \"/dtu/blackhole/06/213542/paperdata/lib_size_flow_model.pt\"\n",
    "\n",
    "os.makedirs(os.path.dirname(flow_model_save_path), exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 2000\n",
    "learning_rate = 1e-3\n",
    "latent_dim = 50\n",
    "drop_prob = 0.1   # Classifier-free guidance dropout probability\n",
    "guidance_scale = 2.0\n",
    "n_steps = 50      # Euler integration steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8180b5d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m adata = ad.read_h5ad(input_file_path)\n\u001b[32m      3\u001b[39m latent = adata.obsm[\u001b[33m\"\u001b[39m\u001b[33mX_latent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m latent_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Library Sizes\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtotal_counts\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m adata.obs:\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# ---- Load Data ----\n",
    "adata = ad.read_h5ad(input_file_path)\n",
    "latent = adata.obsm[\"X_latent\"]\n",
    "latent_tensor = torch.tensor(latent, dtype=torch.float32, device=device)\n",
    "\n",
    "# Library Sizes\n",
    "if \"total_counts\" in adata.obs:\n",
    "    lib_sizes = adata.obs[\"total_counts\"].values\n",
    "else:\n",
    "    lib_sizes = np.array(adata.X.sum(1)).flatten()\n",
    "\n",
    "log_lib_sizes = np.log1p(lib_sizes)\n",
    "log_lib_tensor = torch.tensor(log_lib_sizes, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "# Stats for normalization and sampling\n",
    "lib_min, lib_max = log_lib_tensor.min(), log_lib_tensor.max()\n",
    "lib_mean, lib_std = log_lib_tensor.mean(), log_lib_tensor.std()\n",
    "\n",
    "# Cell Types\n",
    "cell_types = adata.obs[\"cell_type\"].astype(str).values\n",
    "unique_types, inverse_idx = np.unique(cell_types, return_inverse=True)\n",
    "num_cell_types = len(unique_types)\n",
    "cell_type_idx = torch.tensor(inverse_idx, dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Data Shape: {latent.shape}\")\n",
    "print(f\"Library Size: Min={lib_min:.2f}, Max={lib_max:.2f}, Mean={lib_mean:.2f}\")\n",
    "print(f\"Cell Types: {unique_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Flow Matching Classes (OT Path) ----\n",
    "\n",
    "class EmpiricalDistribution(nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"data\", data)\n",
    "    def sample(self, n):\n",
    "        idx = torch.randint(0, len(self.data), (n,), device=self.data.device)\n",
    "        return self.data[idx]\n",
    "\n",
    "class GaussianConditionalProbabilityPath:\n",
    "    def __init__(self, p_data):\n",
    "        self.p_data = p_data\n",
    "    def sample_conditional_path(self, z, t):\n",
    "        # Linear interpolation: t * z + (1-t) * noise\n",
    "        # Note: Train_flow_cfg typically uses x1 (data) and x0 (noise)\n",
    "        # Here we adapt to match the provided notebook logic:\n",
    "        # Target is usually data, Source is noise.\n",
    "        # path: x_t = t * x_1 + (1 - t) * x_0\n",
    "        # vector field u_t = x_1 - x_0\n",
    "        return t * z + (1 - t) * torch.randn_like(z)\n",
    "\n",
    "    def conditional_vector_field(self, x_1, x_0):\n",
    "        return x_1 - x_0\n",
    "\n",
    "# ---- Neural Network (With Library Size) ----\n",
    "\n",
    "class TimeEmbedder(nn.Module):\n",
    "    def __init__(self, embed_dim=32):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim), nn.SiLU(),\n",
    "            nn.Linear(embed_dim, embed_dim), nn.SiLU()\n",
    "        )\n",
    "        self.embed_dim = embed_dim\n",
    "    def forward(self, t):\n",
    "        # Sinusoidal embedding\n",
    "        half_dim = self.embed_dim // 2\n",
    "         emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t * emb[None, :]\n",
    "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=1)\n",
    "        return self.mlp(emb)\n",
    "\n",
    "class NeuralVectorField(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim=256, time_embed_dim=64, lib_min=0, lib_max=10):\n",
    "        super().__init__()\n",
    "        self.x_proj = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.z_proj = nn.Linear(latent_dim, hidden_dim) # Type conditioning\n",
    "        self.time_embedder = TimeEmbedder(time_embed_dim)\n",
    "        \n",
    "        # Library size embedding\n",
    "        self.lib_min = lib_min\n",
    "        self.lib_max = lib_max\n",
    "        self.l_proj = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        in_dim = hidden_dim * 3 + time_embed_dim\n",
    "        self. net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim), nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, z, t, l):\n",
    "        # x: current state, z: type condition, t: time, l: library size\n",
    "        hx = self.x_proj(x)\n",
    "        hz = self.z_proj(z)\n",
    "        ht = self.time_embedder(t)\n",
    "        \n",
    "        # Library embedding (normalize first)\n",
    "        l_norm = (l - self.lib_min) / (self.lib_max - self.lib_min)\n",
    "        # Re-use time embedder logic for continuous scalar l\n",
    "        hl_emb = self.time_embedder.forward(l_norm) \n",
    "        hl = self.l_proj(hl_emb)\n",
    "\n",
    "        h = torch.cat([hx, hz, ht, hl], dim=-1)\n",
    "        return self.net(h)\n",
    "\n",
    "class CellTypeConditioner(nn.Module):\n",
    "    def __init__(self, n_types, latent_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(n_types, latent_dim)\n",
    "    def forward(self, idx):\n",
    "        return self.embed(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Initialize Models ----\n",
    "vf_model = NeuralVectorField(latent_dim, lib_min=lib_min, lib_max=lib_max).to(device)\n",
    "conditioner = CellTypeConditioner(num_cell_types, latent_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(list(vf_model.parameters()) + list(conditioner.parameters()), lr=learning_rate)\n",
    "\n",
    "emp_dist = EmpiricalDistribution(latent_tensor)\n",
    "path = GaussianConditionalProbabilityPath(emp_dist)\n",
    "\n",
    "# Null token for classifier-free guidance\n",
    "z_null = torch.zeros(1, latent_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee743a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training Loop (Flow Matching) ----\n",
    "loss_history = []\n",
    "\n",
    "vf_model.train()\n",
    "conditioner.train()\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    # 1. Sample Batch\n",
    "    idx = torch.randint(0, len(latent_tensor), (batch_size,), device=device)\n",
    "    x_1 = latent_tensor[idx] # Target data\n",
    "    x_0 = torch.randn_like(x_1) # Source noise\n",
    "    \n",
    "    # Conditions\n",
    "    batch_types = cell_type_idx[idx]\n",
    "    batch_libs = log_lib_tensor[idx]\n",
    "\n",
    "    # 2. Sample Time\n",
    "    t = torch.rand(batch_size, 1, device=device)\n",
    "\n",
    "    # 3. Compute Path (Interpolation)\n",
    "    # x_t = t * x_1 + (1-t) * x_0\n",
    "    x_t = t * x_1 + (1 - t) * x_0\n",
    "    \n",
    "    # 4. Compute Target Vector Field\n",
    "    # u_t = x_1 - x_0\n",
    "    u_target = x_1 - x_0\n",
    "\n",
    "    # 5. Conditioning w/ Drop (CFG)\n",
    "    z_emb = conditioner(batch_types)\n",
    "    drop_mask = (torch.rand(batch_size, 1, device=device) < drop_prob)\n",
    "    z_used = torch.where(drop_mask, z_null, z_emb)\n",
    "\n",
    "    # 6. Prediction\n",
    "    v_pred = vf_model(x_t, z_used, t, batch_libs)\n",
    "\n",
    "    # 7. Loss\n",
    "    loss = F.mse_loss(v_pred, u_target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Flow Matching Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b18d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Save Models ----\n",
    "torch.save({\n",
    "    'vf_state': vf_model.state_dict(),\n",
    "    'cond_state': conditioner.state_dict()\n",
    "}, flow_model_save_path)\n",
    "print(f\"Saved models to {flow_model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7301667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# SAMPLING AND DECODING (Based on Victor_evaluation_flow.ipynb)\n",
    "# ==================================================================================\n",
    "\n",
    "# 1. Load VAE for Decoding\n",
    "# We need the VAE to map latents -> gene counts\n",
    "vae = NB_Autoencoder(num_features=adata.n_vars, latent_dim=latent_dim)\n",
    "try:\n",
    "    vae.load_state_dict(torch.load(vae_model_path, map_location=device))\n",
    "    vae.to(device)\n",
    "    vae.eval()\n",
    "    print(\"VAE loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: VAE model not found at {vae_model_path}. Decoding will fail.\")\n",
    "\n",
    "# 2. Euler Integration Simulator (Victor's Logic)\n",
    "class LearnedVectorFieldODE:\n",
    "    def __init__(self, vf_model, conditioner, z_target, l_target, guidance_scale=2.0):\n",
    "        self.vf = vf_model\n",
    "        self.z = conditioner(z_target)\n",
    "        self.l = l_target\n",
    "        self.scale = guidance_scale\n",
    "        self.z_null = torch.zeros_like(self.z)\n",
    "    \n",
    "    def drift(self, x, t):\n",
    "        # Duplicate inputs for Cond and Uncond pass\n",
    "        x_in = torch.cat([x, x], dim=0)\n",
    "        t_in = torch.cat([t, t], dim=0)\n",
    "        l_in = torch.cat([self.l, self.l], dim=0)\n",
    "        z_in = torch.cat([self.z, self.z_null], dim=0)\n",
    "        \n",
    "        v_out = self.vf(x_in, z_in, t_in, l_in)\n",
    "        v_cond, v_uncond = v_out.chunk(2, dim=0)\n",
    "        \n",
    "        # CFG Formula: v = v_uncond + s * (v_cond - v_uncond)\n",
    "        return v_uncond + self.scale * (v_cond - v_uncond)\n",
    "\n",
    "def generate_samples(target_type_idx, num_samples, fix_library_size=True):\n",
    "    vf_model.eval()\n",
    "    conditioner.eval()\n",
    "    \n",
    "    # A. Setup Latents\n",
    "    x = torch.randn(num_samples, latent_dim, device=device) # Noise x0\n",
    "    \n",
    "    # B. Setup Conditions\n",
    "    # 1. Cell Type\n",
    "    type_tensor = torch.full((num_samples,), target_type_idx, dtype=torch.long, device=device)\n",
    "    \n",
    "    # 2. Library Size\n",
    "    # We can either sample from the empirical distribution or fix it for evaluation\n",
    "    if fix_library_size:\n",
    "        # Fix to mean for cleaner evaluation\n",
    "        l_val = lib_mean.item()\n",
    "        l_tensor = torch.full((num_samples, 1), l_val, device=device)\n",
    "    else:\n",
    "        # Sample from normal approx of log-libs\n",
    "        l_tensor = torch.normal(lib_mean.item(), lib_std.item(), (num_samples, 1), device=device)\n",
    "\n",
    "    # C. ODE Integration (Euler)\n",
    "    ode = LearnedVectorFieldODE(vf_model, conditioner, type_tensor, l_tensor, guidance_scale)\n",
    "    dt = 1.0 / n_steps\n",
    "    t = torch.zeros(num_samples, 1, device=device)\n",
    "    \n",
    "    print(f\"Sampling {num_samples} cells (Type {target_type_idx})...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_steps):\n",
    "            v = ode.drift(x, t)\n",
    "            x = x + v * dt\n",
    "            t = t + dt\n",
    "            \n",
    "    return x\n",
    "\n",
    "# 3. Generate Latents\n",
    "target_idx = 0 # Example cell type\n",
    "n_gen = 1000\n",
    "generated_latents = generate_samples(target_idx, n_gen)\n",
    "\n",
    "# 4. Rescaling (Matching Victor's logic)\n",
    "# \"generated_rescaled = (generated_tensor - generated_tensor.mean(dim=0)) / std_gen * std_orig + latent_tensor.mean(dim=0)\"\n",
    "std_orig = latent_tensor.std(dim=0)\n",
    "mean_orig = latent_tensor.mean(dim=0)\n",
    "std_gen = generated_latents.std(dim=0)\n",
    "mean_gen = generated_latents.mean(dim=0)\n",
    "\n",
    "generated_rescaled = (generated_latents - mean_gen) / std_gen * std_orig + mean_orig\n",
    "\n",
    "# 5. Decoding with Fixed Library Size (Victor's Logic)\n",
    "target_lib_size = 1800 # Fixed size for evaluation as per Victor's notebook\n",
    "\n",
    "print(\"Decoding to counts...\")\n",
    "with torch.no_grad():\n",
    "    outputs = vae.decode(generated_rescaled, target_lib_size)\n",
    "    mu = outputs[\"mu\"]\n",
    "    theta = torch.exp(outputs[\"theta\"])\n",
    "    \n",
    "    # Sample from Negative Binomial\n",
    "    nb_dist = NegativeBinomial(mu=mu, theta=theta)\n",
    "    X_gen_counts = nb_dist.sample().cpu().numpy()\n",
    "\n",
    "print(f\"Final Generated Counts Shape: {X_gen_counts.shape}\")\n",
    "\n",
    "# Optional: Save or Plot\n",
    "# sc.AnnData(X=X_gen_counts).write(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
